{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c8720e-70d1-467a-b8a1-e3d601de4912",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df0284-e983-46f5-b0f7-d806b0db46b5",
   "metadata": {},
   "source": [
    "## Load bibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90e88a-6d4a-4538-aad3-84123a98a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc # Garbage Collector\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import textwrap\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "\n",
    "import hdbscan\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display # Import display for better DataFrame rendering\n",
    "from matplotlib.patches import Patch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score,\n",
    "    jaccard_score,\n",
    "    silhouette_score,\n",
    "    silhouette_samples,\n",
    ")\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tqdm.auto import tqdm # For progress bars (preferred over from tqdm import tqdm)\n",
    "\n",
    "import geopandas as gpd\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec3ce7-f938-4ced-9792-26336f2d9f00",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cae31-52d0-4bb7-82ae-b88d236e59b7",
   "metadata": {},
   "source": [
    "### Shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369af271-42e4-4d20-b876-58a4c79b269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_alaska_polygons(gdf):\n",
    "    \"\"\"\n",
    "    Goal:\n",
    "        Remove remote parts of Alaska from the geometry data, \n",
    "        particularly those located far to the west (e.g., small island chains).\n",
    "        \n",
    "    Approach:\n",
    "        - Identify the geometry for Alaska.\n",
    "        - If it's a MultiPolygon, filter out parts whose first coordinate has x > 150.\n",
    "        - If it's a single Polygon and its x > 150, discard it.\n",
    "        - Update the geometry or remove Alaska completely if no valid parts remain.\n",
    "    \n",
    "    Output:\n",
    "        Returns the modified GeoDataFrame with Alaska filtered accordingly.\n",
    "    \"\"\"\n",
    "    # Find the row index for Alaska\n",
    "    alaska_idx = gdf[gdf['NAME'] == 'Alaska'].index\n",
    "\n",
    "    # If Alaska is not present, return the GeoDataFrame unchanged\n",
    "    if alaska_idx.empty:\n",
    "        return gdf\n",
    "\n",
    "    # Extract Alaska's geometry\n",
    "    alaska_geometry = gdf.loc[alaska_idx[0], 'geometry']\n",
    "    updated_geometry = None\n",
    "\n",
    "    if isinstance(alaska_geometry, MultiPolygon):\n",
    "        # Keep only those polygons whose first x-coordinate is <= 150\n",
    "        filtered_polys = [\n",
    "            poly for poly in alaska_geometry.geoms \n",
    "            if poly.exterior.coords[0][0] <= 150\n",
    "        ]\n",
    "        if filtered_polys:\n",
    "            updated_geometry = MultiPolygon(filtered_polys)\n",
    "    elif isinstance(alaska_geometry, Polygon):\n",
    "        # Keep the polygon only if its first x-coordinate is <= 150\n",
    "        if alaska_geometry.exterior.coords[0][0] <= 150:\n",
    "            updated_geometry = alaska_geometry\n",
    "\n",
    "    if updated_geometry is not None:\n",
    "        # Update the geometry in the GeoDataFrame\n",
    "        gdf.loc[alaska_idx, 'geometry'] = updated_geometry\n",
    "    else:\n",
    "        # Remove Alaska from the GeoDataFrame if no valid geometry remains\n",
    "        gdf = gdf.drop(index=alaska_idx)\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba624af-15c6-4689-a982-04e1c29964e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Goal ---\n",
    "# Prepare U.S. state and county geometries for mapping and analysis,\n",
    "# with a specific focus on filtering out distant areas (e.g. Hawaii, Puerto Rico, etc.)\n",
    "# and modifying Alaska to exclude remote island groups.\n",
    "\n",
    "# --- Approach ---\n",
    "# - Load state and county shapefiles from pickle files\n",
    "# - Remove non-continental U.S. territories and Hawaii\n",
    "# - Separate Alaska and the continental U.S. into different GeoDataFrames\n",
    "# - Remove an outlier county (FIPS '02016') from both Alaska and continental counties\n",
    "\n",
    "# --- Output ---\n",
    "# Cleaned GeoDataFrames for:\n",
    "# - state and county shapes for continental U.S.\n",
    "# - state and county shapes for Alaska\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load original shapefiles\n",
    "state_shape = pd.read_pickle('data/original_data/pkl/state.pickle')\n",
    "county_shape = pd.read_pickle('data/original_data/pkl/county.pickle')\n",
    "\n",
    "# Filter Alaska polygons to remove remote parts\n",
    "state_shape = filter_alaska_polygons(state_shape)\n",
    "\n",
    "# Rename county GEOID column to match expected FIPS naming\n",
    "county_shape = county_shape.rename(columns={'GEOID': 'FIPS'})\n",
    "\n",
    "# Filter STATEFP codes for continental U.S. (excluding non-continental territories and Hawaii)\n",
    "filtered_statefp_north_america = state_shape.loc[\n",
    "    ~state_shape['NAME'].isin([\n",
    "        'Hawaii', \n",
    "        'Puerto Rico', \n",
    "        'Commonwealth of the Northern Mariana Islands', \n",
    "        'American Samoa', \n",
    "        'United States Virgin Islands', \n",
    "        'Guam'\n",
    "    ]), \n",
    "    'STATEFP'\n",
    "]\n",
    "\n",
    "# Filter STATEFP for Alaska only\n",
    "filtered_statefp_alaska = state_shape.loc[\n",
    "    state_shape['NAME'] == 'Alaska', \n",
    "    'STATEFP'\n",
    "]\n",
    "\n",
    "# Create separate GeoDataFrames for continental U.S. and Alaska (states)\n",
    "filtered_state_shape_north_america = state_shape[state_shape['STATEFP'].isin(filtered_statefp_north_america)]\n",
    "filtered_state_shape_alaska = state_shape[state_shape['STATEFP'].isin(filtered_statefp_alaska)]\n",
    "\n",
    "# Create separate GeoDataFrames for continental U.S. and Alaska (counties)\n",
    "# Also remove the outlier county with FIPS '02016' (St. Paul Island, Alaska)\n",
    "filtered_county_shape_north_america = county_shape[\n",
    "    county_shape['STATEFP'].isin(filtered_statefp_north_america) & \n",
    "    (county_shape['FIPS'] != '02016')\n",
    "]\n",
    "filtered_county_shape_alaska = county_shape[\n",
    "    county_shape['STATEFP'].isin(filtered_statefp_alaska) & \n",
    "    (county_shape['FIPS'] != '02016')\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55980534-5069-44b4-ae45-a3d9744277ba",
   "metadata": {},
   "source": [
    "### County data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c6dd93-7a31-4a84-a130-d8d56d67a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_master = pd.read_pickle('data/original_data/pkl/county_information.pkl')\n",
    "county_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc43c9d6-e43d-4ef4-8fb1-0090b20328f3",
   "metadata": {},
   "source": [
    "## Visualization of US industry regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226c04a-532e-4f53-89b2-45bf9986cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Goal ---\n",
    "# Assign U.S. states to broader industry regions based on their GEOID,\n",
    "# and identify any states that are associated with more than one region.\n",
    "\n",
    "# --- Approach ---\n",
    "# - Read region classification data from a string into a DataFrame\n",
    "# - Merge this data with the cleaned state geometry DataFrame (filtered_state_shape_north_america)\n",
    "# - Fill in missing regions with 'Other'\n",
    "# - Group by state name and count how many distinct regions each state appears in\n",
    "\n",
    "# --- Output ---\n",
    "# List of states that are assigned to more than one region (if any)\n",
    "\n",
    "# Define region classification data as CSV string\n",
    "data = \"\"\"\n",
    "State ID,GEOID,State,Region\n",
    "IL,17,Illinois,North-Eastern\n",
    "KS,20,Kansas,Southern\n",
    "IN,18,Indiana,North-Eastern\n",
    "MI,26,Michigan,North-Eastern\n",
    "OH,39,Ohio,North-Eastern\n",
    "WI,55,Wisconsin,North-Eastern\n",
    "CT,09,Connecticut,New England\n",
    "ME,23,Maine,The New England\n",
    "MA,25,Massachusetts,New England\n",
    "NH,33,New Hampshire,New England\n",
    "RI,44,Rhode Island,New England\n",
    "VT,50,Vermont,New England\n",
    "DE,10,Delaware,New York and Mid-Atlantic\n",
    "DC,11,District of Columbia,New York and Mid-Atlantic\n",
    "MD,24,Maryland,New York and Mid-Atlantic\n",
    "NJ,34,New Jersey,New York and Mid-Atlantic\n",
    "NY,36,New York,New York and Mid-Atlantic\n",
    "PA,42,Pennsylvania,New York and Mid-Atlantic\n",
    "CA,06,California,Pacific Coastal\n",
    "OR,41,Oregon,Pacific Coastal\n",
    "WA,53,Washington,Pacific Coastal\n",
    "AL,01,Alabama,Southern\n",
    "AR,05,Arkansas,Southern\n",
    "FL,12,Florida,Southern\n",
    "GA,13,Georgia,Southern\n",
    "LA,22,Louisiana,Southern\n",
    "MS,28,Mississippi,Southern\n",
    "OK,40,Oklahoma,Southern\n",
    "TN,47,Tennessee,Southern\n",
    "TX,48,Texas,Southern\n",
    "AZ,04,Arizona,Western\n",
    "CO,08,Colorado,Western\n",
    "NV,32,Nevada,Western\n",
    "NM,35,New Mexico,Western\n",
    "UT,49,Utah,Western\n",
    "WY,56,Wyoming,Western\n",
    "PR,72,Puerto Rico,Other\n",
    "AS,60,American Samoa,Other\n",
    "AK,02,Alaska,Other\n",
    "HI,15,Hawaii,Other\n",
    "GU,66,Guam,Other\n",
    "VI,78,U.S. Virgin Islands,Other\n",
    "MP,69,Northern Mariana Islands,Other\n",
    "\"\"\"\n",
    "\n",
    "# Read the region data into a DataFrame\n",
    "region_df = pd.read_csv(StringIO(data), dtype={'GEOID': str})\n",
    "\n",
    "# Merge with filtered state geometries to assign regions\n",
    "industry_regions = filtered_state_shape_north_america.merge(region_df, on=\"GEOID\", how=\"left\")\n",
    "\n",
    "# Fill any missing regions with 'Other'\n",
    "industry_regions['Region'] = industry_regions['Region'].fillna('Other')\n",
    "\n",
    "# Identify states that appear in more than one region\n",
    "multi_region_states = (\n",
    "    industry_regions.groupby(\"State\")[\"Region\"]\n",
    "    .nunique()\n",
    "    .loc[lambda x: x > 1]\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Output the result\n",
    "print(\"States assigned to multiple regions:\", multi_region_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21482430-8636-417b-aa9b-68ffc5b06816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Goal ---\n",
    "# Identify U.S. territories or states from the master dataset that are \n",
    "# not included in the industry_regions DataFrame (e.g., excluded areas like Puerto Rico or Guam).\n",
    "\n",
    "# --- Approach ---\n",
    "# - Perform a left merge between the county master and the industry regions on state FIPS code\n",
    "# - Use the merge indicator to find unmatched entries (i.e., those only in the county master)\n",
    "# - Extract unique state FIPS codes and state names for the missing entries\n",
    "\n",
    "# --- Output ---\n",
    "# List of U.S. territories or states (statefp and state_name) not found in the industry region data\n",
    "\n",
    "# Perform a left join to compare presence in both datasets\n",
    "schnitttmenge = pd.merge(\n",
    "    county_master[['statefp', 'state_name']],  # Include state_name for context\n",
    "    industry_regions[['STATEFP']],\n",
    "    left_on='statefp',\n",
    "    right_on='STATEFP',\n",
    "    how='left',\n",
    "    indicator=True  # Adds column '_merge' to indicate match status\n",
    ")\n",
    "\n",
    "# Filter for rows that exist only in the county_master (i.e., no match in industry_regions)\n",
    "fehlende_werte = schnitttmenge[schnitttmenge['_merge'] == 'left_only']\n",
    "\n",
    "# Keep only unique statefp and state_name entries\n",
    "unique_fehlende_werte = fehlende_werte[['statefp', 'state_name']].drop_duplicates()\n",
    "\n",
    "# Create a list of the missing statefp values\n",
    "us_territories = unique_fehlende_werte['statefp'].to_list()\n",
    "\n",
    "# Output the missing states/territories\n",
    "print(unique_fehlende_werte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b889a2-76ad-49bf-89f2-65953e04e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Goal ---\n",
    "# Remove FIPS entries from U.S. territories not included in the analysis.\n",
    "# Compare the number of unique FIPS codes before and after filtering.\n",
    "\n",
    "df_original = pd.read_pickle(\"data/processed_data/pkl/feature_df.pkl\")\n",
    "print(df_original['FIPS'].nunique())\n",
    "\n",
    "df_original = df_original[~df_original['FIPS'].str[:2].isin(us_territories)].reset_index(drop=True)\n",
    "print(df_original['FIPS'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d86d81-6af7-4bcd-b192-d569f3446aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54e9f6-8687-4aec-9e88-7ab29f1c182f",
   "metadata": {},
   "source": [
    "# Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a933371-5376-4d68-a45f-e59ed8f256e0",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fae98f-a009-4c5e-a0ed-dbb710e50eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Data Preprocessing for Log10 Scaling ---\n",
    "# Applies Log10 scaling to features, safely handling zeros and NaNs by replacing them\n",
    "# with a small positive value to prevent mathematical errors.\n",
    "\n",
    "# Define columns to be excluded from scaling (e.g., identifiers)\n",
    "columns_to_exclude_from_scaling = ['FIPS']\n",
    "\n",
    "# Extract ID columns\n",
    "df_ids = df_original[columns_to_exclude_from_scaling].copy()\n",
    "\n",
    "# Isolate raw feature columns for scaling\n",
    "df_raw_features = df_original.drop(columns=columns_to_exclude_from_scaling)\n",
    "\n",
    "# Define a small positive value to replace zeros and NaNs before log transformation\n",
    "epsilon_value = 0.001\n",
    "\n",
    "# Replace zeros and NaNs in features with epsilon_value\n",
    "df_features_for_log = df_raw_features.replace(0, epsilon_value).fillna(epsilon_value)\n",
    "\n",
    "# Perform Log10 transformation\n",
    "df_log_transformed_features = np.log10(df_features_for_log)\n",
    "\n",
    "# Ensure transformed data retains original column names and index\n",
    "df_log_transformed_features = pd.DataFrame(\n",
    "    df_log_transformed_features,\n",
    "    columns=df_features_for_log.columns,\n",
    "    index=df_features_for_log.index\n",
    ")\n",
    "\n",
    "# Concatenate ID columns with log-transformed features\n",
    "df_scaled_log = pd.concat([df_ids, df_log_transformed_features], axis=1)\n",
    "\n",
    "print(\"\\nData preprocessing complete. Preview of df_scaled_log:\")\n",
    "display(df_scaled_log.head()) # Use display() for better rendering\n",
    "\n",
    "# Optional: Access to the original (non-scaled) feature values for reference\n",
    "df_original_features = df_original.drop(columns=columns_to_exclude_from_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456597fd-f378-43b8-b5f1-f6b685b8bf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_id_dict = {feature: \"F\" + str(i+1) for i, feature in enumerate(df_original_features.columns)}\n",
    "feature_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c9d47-c525-4779-aed7-8e09c4ee3763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Goal ---\n",
    "# Compare the distribution of feature values before and after log10 scaling using scatter plots.\n",
    "# Visualize each feature as a vertical column of points, labeled with its assigned feature ID.\n",
    "\n",
    "columns_original = df_original.columns[1:]  # Skip first column (e.g., FIPS)\n",
    "df_features = df_original[columns_original]\n",
    "\n",
    "# Assign feature IDs like F1, F2, ... for better readability in plots\n",
    "feature_id_dict = {feature: \"F\" + str(i + 1) for i, feature in enumerate(columns_original)}\n",
    "feature_ids = [feature_id_dict[col] for col in columns_original]\n",
    "\n",
    "# Plot setup\n",
    "ss = 10  # Marker size\n",
    "fig, axs = plt.subplots(1, 2, figsize=(9, 3), sharey=False)\n",
    "\n",
    "# Scatter plot: Original features\n",
    "for i, col in enumerate(columns_original):\n",
    "    axs[0].scatter(np.full(len(df_original[col]), i), df_original[col], alpha=0.5, s=ss)\n",
    "axs[0].set_title('Original Features')\n",
    "axs[0].set_xticks(range(len(columns_original)))\n",
    "axs[0].set_xticklabels(feature_ids, rotation=90)\n",
    "axs[0].set_ylabel('Value')\n",
    "for i, col in enumerate(columns_original):\n",
    "    axs[0].set_ylim(df_original[col].min() - 2000, df_original[col].max() + 2000)\n",
    "\n",
    "# Scatter plot: Log10-scaled features\n",
    "for i, col in enumerate(columns_original):\n",
    "    axs[1].scatter(np.full(len(df_scaled_log[col]), i), df_scaled_log[col], alpha=0.5, s=ss)\n",
    "axs[1].set_title('Log10 Scaled Features')\n",
    "axs[1].set_xticks(range(len(columns_original)))\n",
    "axs[1].set_xticklabels(feature_ids, rotation=90)\n",
    "for i, col in enumerate(columns_original):\n",
    "    axs[1].set_ylim(df_scaled_log[col].min() - 1., df_scaled_log[col].max() + 2)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d78352-463f-4c08-ab0e-ac1ac8d32412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_histograms(df_original, df_scaled_log):\n",
    "    # Get feature columns (excluding the first, e.g., ID column)\n",
    "    columns_original = df_original.columns[1:]\n",
    "    \n",
    "    # Define scaling datasets and their labels\n",
    "    datasets = [\n",
    "        (df_scaled_log, 'Log10 Scaled')\n",
    "    ]\n",
    "    \n",
    "    # Create one row of subplots per feature\n",
    "    for col in columns_original:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        \n",
    "        # Histogram of original values\n",
    "        axs[0].hist(df_original[col], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axs[0].set_title(f'{col} - Original')\n",
    "        axs[0].set_xlabel('Value')\n",
    "        axs[0].set_ylabel('Frequency')\n",
    "        \n",
    "        # Histogram of log10-scaled values\n",
    "        axs[1].hist(df_scaled_log[col], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axs[1].set_title(f'{col} - Log10 Scaled')\n",
    "        axs[1].set_xlabel('Value')\n",
    "        axs[1].set_ylabel('Frequency')\n",
    "\n",
    "              \n",
    "        # Title for the full figure\n",
    "        fig.suptitle(f'Histograms for {col}', fontsize=16)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "plot_histograms(df_original, df_scaled_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5534a0-f355-44ec-91e6-49c194dbefb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_scaled = df_scaled_log\n",
    "df_scaled_features = df_scaled_log.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e99d1a-e297-4cbe-8b13-8c371786e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Goal ---\n",
    "# Create a structured summary table for all features, including metadata, statistics, and data types.\n",
    "# Ensure consistent formatting for display and replace missing values where necessary.\n",
    "\n",
    "# Build summary DataFrame without rounding numerical stats\n",
    "summary = pd.DataFrame({\n",
    "    'Feature': df_original_features.columns,\n",
    "    'ID': [\"F\" + str(i + 1) for i in range(len(df_original_features.columns))],\n",
    "    'Description': [\n",
    "        \"SOC 51-9022, Grinding, Polishing by Hand\",\n",
    "        \"SOC 51-4121, Welders, Cutters, Solderers\",\n",
    "        \"SOC 49-9041, Industrial Machinery Mechanics\",\n",
    "        \"SOC 49-9071, Maintenance and Repair Workers\",\n",
    "        \"SOC 51-4033, Grinding, Lapping, Polishing\",\n",
    "        \"SOC 51-4035, Milling and Planing Machine Setters\",\n",
    "        \"SOC 47-2211, Sheet Metal Workers\",\n",
    "        \"SOC 51-2041, Structural Metal Fabricators\",\n",
    "        \"NAICS 3315, Foundries\", \n",
    "        \"NAICS 3364, Aerospace\",\n",
    "        \"NAICS 3366, Shipbuilding\",\n",
    "        \"NAICS 3335, Metalworking Machines Manufacturing\", \n",
    "        \"NAICS 3320A1, Steel forming\", \n",
    "        \"NAICS 3320A2, Structural Metals Manufacturing\",\n",
    "        \"NAICS 3327, Machine Shops\", \n",
    "        \"NAICS 3312, Steel Product Manufacturing\", \n",
    "        \"NAICS 3314, Nonferrous Metal Production\",\n",
    "        \"NAICS 3361/3362, Automotive\"\n",
    "    ],\n",
    "    'Data Type': df_original_features.dtypes.replace('float64', 'Numeric')\n",
    "                                             .replace('int64', 'Numeric')\n",
    "                                             .replace('object', 'Binary'),\n",
    "    'min': df_original_features.min(),\n",
    "    'max': df_original_features.max(),\n",
    "    'mean': df_original_features.mean(),\n",
    "    'std': df_original_features.std(),\n",
    "    'Number of counties': df_original_features.select_dtypes(include=['number'])\n",
    "                                              .apply(lambda x: (x > 0).sum(), axis=0)\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Replace NaNs with 'N/A' in min, max, mean, std\n",
    "summary[['min', 'max', 'mean', 'std']] = summary[['min', 'max', 'mean', 'std']].fillna('N/A')\n",
    "\n",
    "# Convert min/max to int if possible\n",
    "summary['max'] = summary['max'].apply(lambda x: int(x) if isinstance(x, (int, float)) else x)\n",
    "summary['min'] = summary['min'].apply(lambda x: int(x) if isinstance(x, (int, float)) else x)\n",
    "\n",
    "# Fill missing count values with fallback and convert to integer\n",
    "summary['Number of counties'] = summary['Number of counties'].fillna(3233).astype(int)\n",
    "\n",
    "# Format mean and std to 1 decimal place\n",
    "summary['mean'] = summary['mean'].apply(lambda x: f\"{x:.1f}\" if isinstance(x, (int, float)) else x)\n",
    "summary['std'] = summary['std'].apply(lambda x: f\"{x:.1f}\" if isinstance(x, (int, float)) else x)\n",
    "\n",
    "# Adjust table display style\n",
    "summary = summary.style.set_properties(**{'text-align': 'left'}) \\\n",
    "                       .set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}])\n",
    "\n",
    "# Display styled summary table\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9f00f-5367-41fa-a87b-59b108b62ee1",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382443d-8776-4dc0-b49c-164c9f9755c7",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636275f-66c9-461f-b50d-08247d5d82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trustworthiness(X, X_embedded, n_neighbors=5):\n",
    "    \"\"\"Calculates the trustworthiness.\"\"\"\n",
    "    nn_original = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nn_embedded = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "\n",
    "    nn_original.fit(X)\n",
    "    nn_embedded.fit(X_embedded)\n",
    "\n",
    "    indices_original = nn_original.kneighbors(return_distance=False)\n",
    "    indices_embedded = nn_embedded.kneighbors(return_distance=False)\n",
    "\n",
    "    trust = 0.0\n",
    "    for i in range(len(X)):\n",
    "        U_i = set(indices_embedded[i])\n",
    "        V_i = set(indices_original[i])\n",
    "        K = len(U_i) \n",
    "        rank_sum = 0\n",
    "        for j in U_i:\n",
    "            if j not in V_i:\n",
    "                original_distances = euclidean_distances(X[i:i+1], X)[0]\n",
    "                sorted_indices = np.argsort(original_distances)\n",
    "                rank = np.where(sorted_indices == j)[0][0] \n",
    "                rank_sum += (rank - K)\n",
    "        trust += (1 - (2 / (K * (2 * len(X) - 3 * K - 1))) * rank_sum)\n",
    "    return trust / len(X)\n",
    "\n",
    "\n",
    "def calculate_continuity(X, X_embedded, n_neighbors=5):\n",
    "    \"\"\"Calculates the continuity.\"\"\"\n",
    "    # Continuity is essentially trustworthiness with spaces swapped\n",
    "    return calculate_trustworthiness(X_embedded, X, n_neighbors=n_neighbors)\n",
    "\n",
    "\n",
    "def calculate_stress1(original_distances, embedded_distances):\n",
    "    \"\"\"Calculates Stress-1 (Kruskal's Stress-1).\"\"\"\n",
    "    numerator = np.sum((original_distances - embedded_distances)**2)\n",
    "    denominator = np.sum(original_distances**2)\n",
    "    return np.sqrt(numerator / denominator)\n",
    "\n",
    "def calculate_shepard_diagram_metrics(original_data, embedded_data):\n",
    "    \"\"\"\n",
    "    Calculates distances, Stress-1, and correlation for the Shepard diagram.\n",
    "    Warning: This is computationally intensive for large datasets.\n",
    "    \"\"\"\n",
    "    n_samples = original_data.shape[0]\n",
    "    \n",
    "    if n_samples > 5000:\n",
    "        print(\"Warning: Calculation of all-pairs distances is very computationally intensive for over 5000 samples. Skipping Stress-1 and Shepard correlation.\")\n",
    "        return None, None\n",
    "\n",
    "    original_dist_matrix = euclidean_distances(original_data)\n",
    "    embedded_dist_matrix = euclidean_distances(embedded_data)\n",
    "\n",
    "    triu_indices = np.triu_indices(n_samples, k=1)\n",
    "    original_distances_flat = original_dist_matrix[triu_indices]\n",
    "    embedded_distances_flat = embedded_dist_matrix[triu_indices]\n",
    "\n",
    "    stress1 = calculate_stress1(original_distances_flat, embedded_distances_flat)\n",
    "    \n",
    "    shepard_correlation, _ = spearmanr(original_distances_flat, embedded_distances_flat)\n",
    "    \n",
    "    return stress1, shepard_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084301fe-1560-4e83-b6e4-92111939f5dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T12:51:45.888210Z",
     "iopub.status.busy": "2025-06-13T12:51:45.887697Z",
     "iopub.status.idle": "2025-06-13T12:51:45.893300Z",
     "shell.execute_reply": "2025-06-13T12:51:45.892465Z",
     "shell.execute_reply.started": "2025-06-13T12:51:45.888182Z"
    }
   },
   "source": [
    "### Parameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897776a-4a5f-4ae3-b092-4a990562ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Grid Search Parameters ---\n",
    "\n",
    "# t-SNE Parameters\n",
    "tsne_perplexities = np.arange(5, 51, 5) # 5, 10, ..., 70\n",
    "tsne_learning_rates = np.arange(100, 301, 100) # 100, 200, ..., 500\n",
    "\n",
    "# UMAP Parameters\n",
    "umap_n_neighbors = np.arange(10, 51, 5) # 10, 20, ..., 100\n",
    "# min_dist starting from 0.1 in 20 steps, evenly distributed\n",
    "umap_min_dist_start = 0.1\n",
    "umap_min_dist_end = 0.1 + 19 * 0.01 # 20 steps, so 19 intervals of 0.01\n",
    "umap_min_distances = np.linspace(umap_min_dist_start, umap_min_dist_end, 20).round(3) # Round to 3 decimal places\n",
    "# Overriding the above linspace generation with a specific list of values\n",
    "umap_min_distances = np.array([0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5])\n",
    "\n",
    "# Storage Paths\n",
    "output_dir = \"data/processed_data_pkl_embedding_grid_search_results\"\n",
    "tsne_output_dir = os.path.join(output_dir, \"tsne_results\")\n",
    "umap_output_dir = os.path.join(output_dir, \"umap_results\")\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(tsne_output_dir, exist_ok=True)\n",
    "os.makedirs(umap_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ad10f-4bf4-48b3-9c3c-13806156daa4",
   "metadata": {},
   "source": [
    "### t-SNE Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fc4d31-7ff7-4ea4-8f0a-0d22e4418e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_tsne_grid_search(dataframe_scaled, tsne_perplexities, tsne_learning_rates, tsne_output_dir):\n",
    "    \"\"\"\n",
    "    Performs a grid search for t-SNE embeddings, evaluating different perplexity and learning rate values.\n",
    "\n",
    "    Args:\n",
    "        dataframe_scaled (pd.DataFrame): The scaled input data for t-SNE.\n",
    "        tsne_perplexities (list): A list of perplexity values to test.\n",
    "        tsne_learning_rates (list): A list of learning rate values to test.\n",
    "        tsne_output_dir (str): The directory to save the results for each perplexity.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains the results\n",
    "              (including metrics) for a specific t-SNE configuration.\n",
    "    \"\"\"\n",
    "    # --- t-SNE Grid Search ---\n",
    "    print(\"\\nStarting t-SNE Grid Search...\")\n",
    "    tsne_results = []\n",
    "\n",
    "    # Calculate total iterations for the progress bar\n",
    "    total_iterations = len(tsne_perplexities) * len(tsne_learning_rates)\n",
    "\n",
    "    with tqdm(total=total_iterations, desc=\"t-SNE Grid Search\") as pbar:\n",
    "        for perplexity in tsne_perplexities:\n",
    "            current_perplexity_results = []\n",
    "            # The perplexity processing message can remain, as it's more specific than the overall progress bar\n",
    "            print(f\"\\nProcessing t-SNE Perplexity: {perplexity}\") \n",
    "            for lr in tsne_learning_rates:\n",
    "                # The learning rate message can also remain for detailed logging\n",
    "                print(f\"  Learning Rate: {lr}\")\n",
    "                try:\n",
    "                    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate=lr,\n",
    "                                random_state=42, n_jobs=-1,  # Utilize all available CPU cores if possible\n",
    "                                init='pca')  # PCA initialization is often more stable\n",
    "                    tsne_embedding = tsne.fit_transform(dataframe_scaled)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    # These functions (calculate_trustworthiness, calculate_continuity, calculate_shepard_diagram_metrics)\n",
    "                    # are assumed to be defined elsewhere and accessible.\n",
    "                    trustworthiness = calculate_trustworthiness(dataframe_scaled.values, tsne_embedding)\n",
    "                    continuity = calculate_continuity(dataframe_scaled.values, tsne_embedding)\n",
    "                    \n",
    "                    # Shepard Diagram Metrics (Stress-1, Correlation)\n",
    "                    stress1, shepard_correlation = calculate_shepard_diagram_metrics(dataframe_scaled.values, tsne_embedding)\n",
    "\n",
    "                    current_perplexity_results.append({\n",
    "                        'algorithm': 't-SNE',\n",
    "                        'perplexity': perplexity,\n",
    "                        'learning_rate': lr,\n",
    "                        'embedding_shape': tsne_embedding.shape,\n",
    "                        'trustworthiness': trustworthiness,\n",
    "                        'continuity': continuity,\n",
    "                        'stress_1': stress1,\n",
    "                        'shepard_correlation': shepard_correlation\n",
    "                    })\n",
    "                    print(f\"    Trustworthiness: {trustworthiness:.4f}, Stress-1: {stress1:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during t-SNE (perplexity={perplexity}, lr={lr}): {e}\")\n",
    "                    current_perplexity_results.append({\n",
    "                        'algorithm': 't-SNE',\n",
    "                        'perplexity': perplexity,\n",
    "                        'learning_rate': lr,\n",
    "                        'embedding_shape': None,\n",
    "                        'trustworthiness': None,\n",
    "                        'continuity': None,\n",
    "                        'stress_1': None,\n",
    "                        'shepard_correlation': None,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                gc.collect()  # Release memory\n",
    "                pbar.update(1) # Update the progress bar after each iteration\n",
    "\n",
    "            # Save results for the current perplexity\n",
    "            tsne_perplexity_df = pd.DataFrame(current_perplexity_results)\n",
    "            file_name = f\"tsne_perplexity_{perplexity}.pkl\"\n",
    "            file_path = os.path.join(tsne_output_dir, file_name)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(tsne_perplexity_df, f)\n",
    "            print(f\"  Results for Perplexity {perplexity} saved to {file_path}\")\n",
    "            tsne_results.extend(current_perplexity_results)  # For the final summary\n",
    "            \n",
    "    return tsne_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3469ce-6b1a-4a0e-bc6a-12066884182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d412fc-d92e-43a0-8426-4cb30ab3a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results = perform_tsne_grid_search(df_scaled.iloc[:, 1:-4], tsne_perplexities, tsne_learning_rates, tsne_output_dir)\n",
    "print(\"\\nAll t-SNE results collected:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edac794-c776-44f9-a2e0-05d5559d0d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4081e3e-b63b-43bb-8a87-b4b63e25f94a",
   "metadata": {},
   "source": [
    "### UMAP Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf659cd-08b5-47ab-89e6-32a76d37072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_umap_grid_search(dataframe_scaled, umap_n_neighbors, umap_min_distances, umap_output_dir):\n",
    "    \"\"\"\n",
    "    Performs a grid search for UMAP embeddings, evaluating different n_neighbors and min_dist values.\n",
    "\n",
    "    Args:\n",
    "        dataframe_scaled (pd.DataFrame): The scaled input data for UMAP.\n",
    "        umap_n_neighbors (list): A list of n_neighbors values to test.\n",
    "        umap_min_distances (list): A list of min_dist values to test.\n",
    "        umap_output_dir (str): The directory to save the results for each n_neighbors.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains the results\n",
    "              (including metrics) for a specific UMAP configuration.\n",
    "    \"\"\"\n",
    "    # --- UMAP Grid Search ---\n",
    "    print(\"\\nStarting UMAP Grid Search...\")\n",
    "    umap_results = []\n",
    "\n",
    "    # Calculate total iterations for the progress bar\n",
    "    total_iterations = len(umap_n_neighbors) * len(umap_min_distances)\n",
    "\n",
    "    with tqdm(total=total_iterations, desc=\"UMAP Grid Search\") as pbar:\n",
    "        for n_neighbors in umap_n_neighbors:\n",
    "            current_n_neighbors_results = []\n",
    "            # The n_neighbors processing message can remain, as it's more specific than the overall progress bar\n",
    "            print(f\"\\nProcessing UMAP n_neighbors: {n_neighbors}\")\n",
    "            for min_dist in umap_min_distances:\n",
    "                # The minimum distance message can also remain for detailed logging\n",
    "                print(f\"  Minimum Distance: {min_dist}\")\n",
    "                try:\n",
    "                    umap_model = UMAP(n_components=2, n_neighbors=n_neighbors, min_dist=min_dist,\n",
    "                                      random_state=42, n_jobs=-1) # Utilize all available CPU cores\n",
    "                    umap_embedding = umap_model.fit_transform(dataframe_scaled)\n",
    "\n",
    "                    # Calculate metrics\n",
    "                    # These functions (calculate_trustworthiness, calculate_continuity, calculate_shepard_diagram_metrics)\n",
    "                    # are assumed to be defined elsewhere and accessible.\n",
    "                    trustworthiness = calculate_trustworthiness(dataframe_scaled.values, umap_embedding)\n",
    "                    continuity = calculate_continuity(dataframe_scaled.values, umap_embedding)\n",
    "                    \n",
    "                    # Shepard Diagram Metrics\n",
    "                    stress1, shepard_correlation = calculate_shepard_diagram_metrics(dataframe_scaled.values, umap_embedding)\n",
    "\n",
    "                    current_n_neighbors_results.append({\n",
    "                        'algorithm': 'UMAP',\n",
    "                        'n_neighbors': n_neighbors,\n",
    "                        'min_dist': min_dist,\n",
    "                        'embedding_shape': umap_embedding.shape,\n",
    "                        'trustworthiness': trustworthiness,\n",
    "                        'continuity': continuity,\n",
    "                        'stress_1': stress1,\n",
    "                        'shepard_correlation': shepard_correlation\n",
    "                    })\n",
    "                    print(f\"    Trustworthiness: {trustworthiness:.4f}, Stress-1: {stress1:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during UMAP (n_neighbors={n_neighbors}, min_dist={min_dist}): {e}\")\n",
    "                    current_n_neighbors_results.append({\n",
    "                        'algorithm': 'UMAP',\n",
    "                        'n_neighbors': n_neighbors,\n",
    "                        'min_dist': min_dist,\n",
    "                        'embedding_shape': None,\n",
    "                        'trustworthiness': None,\n",
    "                        'continuity': None,\n",
    "                        'stress_1': None,\n",
    "                        'shepard_correlation': None,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                gc.collect() # Release memory\n",
    "                pbar.update(1) # Update the progress bar after each iteration\n",
    "\n",
    "            # Save results for the current n_neighbors\n",
    "            umap_n_neighbors_df = pd.DataFrame(current_n_neighbors_results)\n",
    "            file_name = f\"umap_n_neighbors_{n_neighbors}.pkl\"\n",
    "            file_path = os.path.join(umap_output_dir, file_name)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(umap_n_neighbors_df, f)\n",
    "            print(f\"  Results for n_neighbors {n_neighbors} saved to {file_path}\")\n",
    "            umap_results.extend(current_n_neighbors_results) # For the final summary\n",
    "            \n",
    "    return umap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865e5c8-3c94-4f8a-994b-b8a49a58c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#umap_results = perform_umap_grid_search(df_scaled.iloc[:, 1:-4], umap_n_neighbors, umap_min_distances, umap_output_dir)\n",
    "#print(\"\\nAll t-SNE results collected:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6eece6-b0b9-4d02-b87a-1e2bb986fefd",
   "metadata": {},
   "source": [
    "### Loading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f4a08-9d93-4154-96bf-3ffa65140e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output directories (assuming these are already defined as in your context)\n",
    "output_dir = \"data/processed_data/pkl/embedding_grid_search_results\"\n",
    "tsne_output_dir = os.path.join(output_dir, \"tsne_results\")\n",
    "umap_output_dir = os.path.join(output_dir, \"umap_results\")\n",
    "\n",
    "\n",
    "# --- Consolidate Results ---\n",
    "print(\"\\nConsolidating all results...\")\n",
    "\n",
    "# List to store individual t-SNE DataFrames\n",
    "list_tsne_dfs = []\n",
    "# Iterate through files in the t-SNE output directory\n",
    "for file_name in os.listdir(tsne_output_dir):\n",
    "    # Process only pickle files\n",
    "    if file_name.endswith(\".pkl\"):\n",
    "        file_path = os.path.join(tsne_output_dir, file_name)\n",
    "        # Open and load each pickle file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            df_part = pickle.load(f)\n",
    "            # Only append if the DataFrame is not empty\n",
    "            if not df_part.empty:\n",
    "                list_tsne_dfs.append(df_part)\n",
    "\n",
    "# Concatenate all t-SNE DataFrames from the list at once\n",
    "if list_tsne_dfs: # Only concatenate if the list of non-empty DataFrames is not empty\n",
    "    final_tsne_df = pd.concat(list_tsne_dfs, ignore_index=True)\n",
    "else:\n",
    "    # Define columns explicitly if starting with an empty DataFrame\n",
    "    # This assumes all your results DataFrames have these columns.\n",
    "    # Adjust this list based on your actual column names.\n",
    "    default_columns = [\n",
    "        'algorithm', 'perplexity', 'learning_rate', 'embedding_shape',\n",
    "        'trustworthiness', 'continuity', 'stress_1', 'shepard_correlation', 'error',\n",
    "        'n_neighbors', 'min_dist' # Include UMAP specific columns as well for overall compatibility\n",
    "    ]\n",
    "    final_tsne_df = pd.DataFrame(columns=default_columns) # Initialize with known columns\n",
    "\n",
    "# List to store individual UMAP DataFrames\n",
    "list_umap_dfs = []\n",
    "# Iterate through files in the UMAP output directory\n",
    "for file_name in os.listdir(umap_output_dir):\n",
    "    # Process only pickle files\n",
    "    if file_name.endswith(\".pkl\"):\n",
    "        file_path = os.path.join(umap_output_dir, file_name)\n",
    "        # Open and load each pickle file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            df_part = pickle.load(f)\n",
    "            # Only append if the DataFrame is not empty\n",
    "            if not df_part.empty:\n",
    "                list_umap_dfs.append(df_part)\n",
    "\n",
    "# Concatenate all UMAP Dataframes from the list at once\n",
    "if list_umap_dfs: # Only concatenate if the list of non-empty DataFrames is not empty\n",
    "    final_umap_df = pd.concat(list_umap_dfs, ignore_index=True)\n",
    "else:\n",
    "    # Define columns explicitly if starting with an empty DataFrame\n",
    "    default_columns = [\n",
    "        'algorithm', 'perplexity', 'learning_rate', 'embedding_shape',\n",
    "        'trustworthiness', 'continuity', 'stress_1', 'shepard_correlation', 'error',\n",
    "        'n_neighbors', 'min_dist'\n",
    "    ]\n",
    "    final_umap_df = pd.DataFrame(columns=default_columns) # Initialize with known columns\n",
    "\n",
    "\n",
    "# Concatenate t-SNE and UMAP results into a single final DataFrame\n",
    "# This specific concat operation is the one that might still warn if one of them is empty.\n",
    "# By ensuring they start with defined columns if empty, it helps.\n",
    "final_results_df = pd.concat([final_tsne_df, final_umap_df], ignore_index=True)\n",
    "\n",
    "# Define path for the final consolidated output file\n",
    "final_output_file = os.path.join(output_dir, \"all_embedding_results.pkl\")\n",
    "# Save the final DataFrame to a pickle file\n",
    "with open(final_output_file, 'wb') as f:\n",
    "    pickle.dump(final_results_df, f)\n",
    "\n",
    "print(f\"\\nAll results successfully consolidated and saved to {final_output_file}.\")\n",
    "print(\"\\nFinal DataFrame (first 5 rows):\")\n",
    "# Display the first 5 rows of the final DataFrame\n",
    "try:\n",
    "    display(final_results_df.head())\n",
    "except NameError:\n",
    "    print(final_results_df.head())\n",
    "\n",
    "print(f\"\\nNumber of t-SNE results: {len(final_tsne_df)}\")\n",
    "print(f\"Number of UMAP results: {len(final_umap_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef69494e-24ac-4a40-a011-f407f4a21d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "output_dir = \"data/processed_data/pkl/embedding_grid_search_results\"\n",
    "final_output_file = os.path.join(output_dir, \"all_embedding_results.pkl\")\n",
    "\n",
    "# --- Load Results ---\n",
    "try:\n",
    "    with open(final_output_file, 'rb') as f:\n",
    "        results_df = pickle.load(f)\n",
    "    print(f\"Results successfully loaded from '{final_output_file}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{final_output_file}' was not found. Please ensure the previous step has been executed.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(f\"Total number of entries: {len(results_df)}\")\n",
    "\n",
    "# --- Clean up failed runs ---\n",
    "# Remove rows where 'embedding_shape' is None (indicator for errors)\n",
    "results_df_cleaned = results_df.dropna(subset=['embedding_shape', 'trustworthiness', 'continuity', 'shepard_correlation'])\n",
    "\n",
    "# --- Calculate Ranking and Mean Rank ---\n",
    "\n",
    "# Metrics where higher values are better (ascending=False for ranking)\n",
    "positive_metrics = ['trustworthiness', 'continuity', 'shepard_correlation']\n",
    "# Metrics where lower values are better (ascending=True for ranking)\n",
    "negative_metrics = ['stress_1']\n",
    "\n",
    "# DataFrame for ranks\n",
    "ranked_results_df = results_df_cleaned.copy()\n",
    "\n",
    "# Ranks for t-SNE\n",
    "tsne_df = ranked_results_df[ranked_results_df['algorithm'] == 't-SNE'].copy()\n",
    "for metric in positive_metrics:\n",
    "    # Note that 'stress_1' should not be included here, which is automatically handled as it's not in positive_metrics\n",
    "    tsne_df[f'{metric}_rank'] = tsne_df[metric].rank(method='average', ascending=False)\n",
    "for metric in negative_metrics:\n",
    "    # Stress-1 is usually ranked here, but not included in the mean rank for t-SNE\n",
    "    tsne_df[f'{metric}_rank'] = tsne_df[metric].rank(method='average', ascending=True)\n",
    "\n",
    "# Columns for the mean rank for t-SNE\n",
    "# Explicitly exclude metrics that should not be considered for t-SNE's mean rank\n",
    "tsne_metrics_for_mean_rank = [f'{m}_rank' for m in positive_metrics] + \\\n",
    "                             [f'{m}_rank' for m in negative_metrics] \n",
    "                             \n",
    "tsne_df['mean_rank'] = tsne_df[tsne_metrics_for_mean_rank].mean(axis=1)\n",
    "\n",
    "# Ranks for UMAP\n",
    "umap_df = ranked_results_df[ranked_results_df['algorithm'] == 'UMAP'].copy()\n",
    "for metric in positive_metrics:\n",
    "    umap_df[f'{metric}_rank'] = umap_df[metric].rank(method='average', ascending=False)\n",
    "for metric in negative_metrics:\n",
    "    umap_df[f'{metric}_rank'] = umap_df[metric].rank(method='average', ascending=True)\n",
    "\n",
    "# Columns for the mean rank for UMAP (all metrics are considered here)\n",
    "umap_metrics_for_mean_rank = [f'{m}_rank' for m in positive_metrics] + \\\n",
    "                             [f'{m}_rank' for m in negative_metrics]\n",
    "\n",
    "umap_df['mean_rank'] = umap_df[umap_metrics_for_mean_rank].mean(axis=1)\n",
    "\n",
    "# --- Find Best Parameters ---\n",
    "\n",
    "print(\"\\n--- Best Parameters for t-SNE ---\")\n",
    "best_tsne_params = tsne_df.sort_values(by='mean_rank').iloc[0]\n",
    "print(f\"Best t-SNE Parameters (Mean Rank: {best_tsne_params['mean_rank']:.2f}):\")\n",
    "print(f\"  Perplexity: {best_tsne_params['perplexity']}\")\n",
    "print(f\"  Learning Rate: {best_tsne_params['learning_rate']}\")\n",
    "print(\"  Metrics for this combination:\")\n",
    "print(f\"    Trustworthiness: {best_tsne_params['trustworthiness']:.4f} (Rank: {best_tsne_params['trustworthiness_rank']:.2f})\")\n",
    "print(f\"    Continuity: {best_tsne_params['continuity']:.4f} (Rank: {best_tsne_params['continuity_rank']:.2f})\")\n",
    "print(f\"    Shepard Correlation: {best_tsne_params['shepard_correlation']:.4f} (Rank: {best_tsne_params['shepard_correlation_rank']:.2f})\")\n",
    "print(f\"    Stress-1 (not used for ranking): {best_tsne_params['stress_1']:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Best Parameters for UMAP ---\")\n",
    "best_umap_params = umap_df.sort_values(by='mean_rank').iloc[0]\n",
    "print(f\"Best UMAP Parameters (Mean Rank: {best_umap_params['mean_rank']:.2f}):\")\n",
    "print(f\"  n_neighbors: {best_umap_params['n_neighbors']}\")\n",
    "print(f\"  min_dist: {best_umap_params['min_dist']}\")\n",
    "print(\"  Metrics for this combination:\")\n",
    "print(f\"    Trustworthiness: {best_umap_params['trustworthiness']:.4f} (Rank: {best_umap_params['trustworthiness_rank']:.2f})\")\n",
    "print(f\"    Continuity: {best_umap_params['continuity']:.4f} (Rank: {best_umap_params['continuity_rank']:.2f})\")\n",
    "print(f\"    Shepard Correlation: {best_umap_params['shepard_correlation']:.4f} (Rank: {best_umap_params['shepard_correlation_rank']:.2f})\")\n",
    "print(f\"    Stress-1: {best_umap_params['stress_1']:.4f} (Rank: {best_umap_params['stress_1_rank']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8dd64-2698-491c-ade8-424d07ef18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_scaled = df_scaled.iloc[:, 1:]\n",
    "# --- Laden der vorherigen Grid Search Ergebnisse ---\n",
    "output_dir = \"data/processed_data/pkl/embedding_grid_search_results\"\n",
    "final_output_file = os.path.join(output_dir, \"all_embedding_results.pkl\")\n",
    "\n",
    "try:\n",
    "    with open(final_output_file, 'rb') as f:\n",
    "        results_df = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Fehler: Die Datei '{final_output_file}' wurde nicht gefunden. Bitte stellen Sie sicher, dass der vorherige Schritt ausgeführt wurde.\")\n",
    "    exit()\n",
    "\n",
    "# --- Bereinigung der Ergebnisse und Bestimmung der besten t-SNE und UMAP Parameter ---\n",
    "results_df_cleaned = results_df.dropna(subset=['embedding_shape', 'trustworthiness', 'continuity', 'shepard_correlation'])\n",
    "\n",
    "# Metriken, bei denen höhere Werte besser sind (ascending=False für Ranking)\n",
    "positive_metrics = ['trustworthiness', 'continuity', 'shepard_correlation']\n",
    "# Metriken, bei denen niedrigere Werte besser sind (ascending=True für Ranking)\n",
    "negative_metrics = ['stress_1']\n",
    "\n",
    "# t-SNE besten Parameter finden\n",
    "tsne_df = results_df_cleaned[results_df_cleaned['algorithm'] == 't-SNE'].copy()\n",
    "for metric in positive_metrics:\n",
    "    tsne_df[f'{metric}_rank'] = tsne_df[metric].rank(method='average', ascending=False)\n",
    "for metric in negative_metrics: # Stress-1 wird hier gerankt, aber nicht in den mittleren Rang einbezogen\n",
    "    tsne_df[f'{metric}_rank'] = tsne_df[metric].rank(method='average', ascending=True)\n",
    "\n",
    "tsne_metrics_for_mean_rank = [f'{m}_rank' for m in positive_metrics] + \\\n",
    "                             [f'{m}_rank' for m in negative_metrics if m != 'stress_1'] # Stress-1 ausschließen\n",
    "tsne_df['mean_rank'] = tsne_df[tsne_metrics_for_mean_rank].mean(axis=1)\n",
    "best_tsne_params = tsne_df.sort_values(by='mean_rank').iloc[0]\n",
    "\n",
    "# UMAP besten Parameter finden\n",
    "umap_df = results_df_cleaned[results_df_cleaned['algorithm'] == 'UMAP'].copy()\n",
    "for metric in positive_metrics:\n",
    "    umap_df[f'{metric}_rank'] = umap_df[metric].rank(method='average', ascending=False)\n",
    "for metric in negative_metrics:\n",
    "    umap_df[f'{metric}_rank'] = umap_df[metric].rank(method='average', ascending=True)\n",
    "\n",
    "umap_metrics_for_mean_rank = [f'{m}_rank' for m in positive_metrics] + \\\n",
    "                             [f'{m}_rank' for m in negative_metrics] # Alle Metriken einbeziehen\n",
    "umap_df['mean_rank'] = umap_df[umap_metrics_for_mean_rank].mean(axis=1)\n",
    "best_umap_params = umap_df.sort_values(by='mean_rank').iloc[0]\n",
    "\n",
    "# --- PCA Berechnung und Metriken ---\n",
    "print(\"\\nBerechne PCA-Einbettung und Metriken...\")\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_embedding = pca.fit_transform(dataframe_scaled)\n",
    "\n",
    "pca_trustworthiness = calculate_trustworthiness(dataframe_scaled.values, pca_embedding)\n",
    "pca_continuity = calculate_continuity(dataframe_scaled.values, pca_embedding)\n",
    "pca_stress1, pca_shepard_correlation = calculate_shepard_diagram_metrics(dataframe_scaled.values, pca_embedding)\n",
    "\n",
    "pca_results = {\n",
    "    'algorithm': 'PCA',\n",
    "    'trustworthiness': pca_trustworthiness,\n",
    "    'continuity': pca_continuity,\n",
    "    'stress_1': pca_stress1,\n",
    "    'shepard_correlation': pca_shepard_correlation\n",
    "}\n",
    "\n",
    "# --- Finale Tabelle erstellen ---\n",
    "final_table_data = []\n",
    "\n",
    "# Daten für PCA\n",
    "final_table_data.append({\n",
    "    'Method': 'PCA',\n",
    "    'Trustworthiness': pca_results['trustworthiness'],\n",
    "    'Continuity': pca_results['continuity'],\n",
    "    'Shepard Correlation': pca_results['shepard_correlation'],\n",
    "    'Stress-1': pca_results['stress_1']\n",
    "})\n",
    "\n",
    "# Daten für t-SNE (beste Parameter)\n",
    "final_table_data.append({\n",
    "    'Method': 't-SNE',\n",
    "    'Trustworthiness': best_tsne_params['trustworthiness'],\n",
    "    'Continuity': best_tsne_params['continuity'],\n",
    "    'Shepard Correlation': best_tsne_params['shepard_correlation'],\n",
    "    'Stress-1': best_tsne_params['stress_1']\n",
    "})\n",
    "\n",
    "# Daten für UMAP (beste Parameter)\n",
    "final_table_data.append({\n",
    "    'Method': 'UMAP',\n",
    "    'Trustworthiness': best_umap_params['trustworthiness'],\n",
    "    'Continuity': best_umap_params['continuity'],\n",
    "    'Shepard Correlation': best_umap_params['shepard_correlation'],\n",
    "    'Stress-1': best_umap_params['stress_1']\n",
    "})\n",
    "\n",
    "final_comparison_df = pd.DataFrame(final_table_data)\n",
    "\n",
    "print(\"\\n--- Finale Vergleichstabelle der besten Parameter für jede Methode ---\")\n",
    "print(final_comparison_df.round(4).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb89bd0d-6e0e-44d0-860d-b87e09532885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate Embeddings\n",
    "X_pca = PCA(n_components=2, random_state=42).fit_transform(df_scaled_features)\n",
    "X_tsne = TSNE(n_components=2,\n",
    "              perplexity=35,\n",
    "              learning_rate=200,\n",
    "              random_state=42).fit_transform(df_scaled_features)\n",
    "X_umap = UMAP(n_components=2,\n",
    "              n_neighbors=20,\n",
    "              min_dist=0.4,\n",
    "              random_state=42).fit_transform(df_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c0ec3-c698-4b72-b06a-27f44dad2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Subplots erstellen ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Variables for plot settings\n",
    "s = 2         # Point size\n",
    "legend_fontsize = 18 # Font size for the legend text\n",
    "\n",
    "# --- PCA-Plot ---\n",
    "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], s=s, color='black', alpha=0.7)\n",
    "# Remove title, add legend\n",
    "axes[0].legend([\"PCA\"], loc='lower right', fontsize=legend_fontsize, frameon=False)\n",
    "axes[0].set_xticks([])\n",
    "axes[0].set_yticks([])\n",
    "\n",
    "# --- t-SNE-Plot ---\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], s=s, color='black', alpha=0.7)\n",
    "# Remove title, add legend with parameters\n",
    "axes[1].legend([\"t-SNE\"], loc='lower right', fontsize=legend_fontsize, frameon=False)\n",
    "axes[1].set_xticks([])\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "# --- UMAP-Plot ---\n",
    "axes[2].scatter(X_umap[:, 0], X_umap[:, 1], s=s, color='black', alpha=0.7)\n",
    "# Remove title, add legend with parameters\n",
    "axes[2].legend([\"UMAP\"], loc='lower right', fontsize=legend_fontsize, frameon=False)\n",
    "axes[2].set_xticks([])\n",
    "axes[2].set_yticks([])\n",
    "\n",
    "plt.tight_layout(pad=0.0)\n",
    "\n",
    "# --- Save Image ---\n",
    "# Save the image with 600 DPI\n",
    "plt.savefig(\"dimensionality_reduction_plots.png\", dpi=600, bbox_inches='tight')\n",
    "\n",
    "\n",
    "#plt.suptitle(\"Dimensionality Reduction Techniques Comparison\", fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b1409-2574-4fc7-a9cb-2627cb31486c",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132dc5d9-576b-4001-9449-b7462a4482e0",
   "metadata": {},
   "source": [
    "## HDBScan (full feature space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadb8bb-5a30-4205-9f1a-ea31b6ceddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_scaled is already defined and loaded\n",
    "data_scaled = df_scaled.iloc[:, 1:].values # .values to ensure it's a NumPy array\n",
    "\n",
    "# --- HDBSCAN Parameter Grid ---\n",
    "hdbscan_params = {\n",
    "    'min_cluster_size': range(10, 50, 2),  # Example values\n",
    "    'min_samples': range(1, 20, 1)        # Example values\n",
    "}\n",
    "\n",
    "# --- Initialization for Results ---\n",
    "hdbscan_raw_results = [] # To store all RAW results for analysis\n",
    "\n",
    "# --- Perform Grid Search ---\n",
    "total_iterations = len(hdbscan_params['min_cluster_size']) * len(hdbscan_params['min_samples'])\n",
    "with tqdm(total=total_iterations, desc=\"HDBSCAN Grid Search (Phase 1: Data Collection)\") as pbar:\n",
    "    for min_cluster_size in hdbscan_params['min_cluster_size']:\n",
    "        for min_samples in hdbscan_params['min_samples']:\n",
    "            hdbscan_model = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=int(min_cluster_size),\n",
    "                min_samples=int(min_samples),\n",
    "                cluster_selection_method='eom', # Explicitly 'eom' for cluster_persistence_\n",
    "                allow_single_cluster=True # Allows all non-noise points to be in a single cluster\n",
    "            )\n",
    "            labels = hdbscan_model.fit_predict(data_scaled)\n",
    "\n",
    "            # Determine the number of unique clusters (excluding noise)\n",
    "            unique_clusters = len(set(labels) - {-1})\n",
    "\n",
    "            # Consider only points assigned to a cluster (not noise)\n",
    "            clustered_points_indices = labels != -1\n",
    "\n",
    "            current_silhouette = -1.0\n",
    "            current_negative_silhouette_ratio = 1.0\n",
    "            current_hdbscan_persistence = 0.0 # Default value if not calculable\n",
    "\n",
    "            # Check: At least 2 assigned points AND at least 2 clusters for Silhouette Score\n",
    "            if np.sum(clustered_points_indices) >= 2 and unique_clusters >= 2:\n",
    "                current_silhouette = silhouette_score(data_scaled[clustered_points_indices], labels[clustered_points_indices])\n",
    "                \n",
    "                silhouette_vals = silhouette_samples(data_scaled[clustered_points_indices], labels[clustered_points_indices])\n",
    "                current_negative_silhouette_ratio = np.sum(silhouette_vals < 0) / len(silhouette_vals)\n",
    "            \n",
    "            # cluster_persistence_ is only calculated if cluster_selection_method='eom' (default)\n",
    "            # and if clusters were found. If only noise or a single cluster, it is None or empty.\n",
    "            if hdbscan_model.cluster_persistence_ is not None and len(hdbscan_model.cluster_persistence_) > 0:\n",
    "                current_hdbscan_persistence = np.mean(hdbscan_model.cluster_persistence_)\n",
    "            else:\n",
    "                current_hdbscan_persistence = 0.0 # Set to 0 if persistence not calculable\n",
    "\n",
    "            # Noise statistics\n",
    "            noise_count = np.sum(labels == -1)\n",
    "            current_noise_ratio = noise_count / len(labels)\n",
    "\n",
    "            # Store results (still WITHOUT combined score, as persistence not yet normalized)\n",
    "            hdbscan_raw_results.append({\n",
    "                'min_cluster_size': int(min_cluster_size),\n",
    "                'min_samples': int(min_samples),\n",
    "                'unique_clusters': unique_clusters,\n",
    "                'silhouette_avg': current_silhouette,\n",
    "                'negative_silhouette_ratio': current_negative_silhouette_ratio,\n",
    "                'noise_ratio': current_noise_ratio,\n",
    "                'hdbscan_persistence': current_hdbscan_persistence # Store raw value\n",
    "            })\n",
    "            \n",
    "            pbar.update(1) # Update progress bar\n",
    "\n",
    "# Convert raw data to a DataFrame\n",
    "hdbscan_results_df = pd.DataFrame(hdbscan_raw_results)\n",
    "hdbscan_results_df.to_pickle('data/processed_data/pkl/cluster_results/hdbscan_grid_search_results.pickle')\n",
    "print(\"\\nAll Grid Search results saved to 'data/processed_data/pkl/cluster_results/hdbscan_grid_search_results.pickle'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4c557-f966-4784-a77b-0463873a3a49",
   "metadata": {},
   "source": [
    "## HDBScan (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ffc60c-8b59-47a4-a3ab-bf06d18f6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = X_tsne\n",
    "\n",
    "# --- HDBSCAN Parameter Grid ---\n",
    "hdbscan_params = {\n",
    "    'min_cluster_size': range(10, 50, 5),  # Example values\n",
    "    'min_samples': range(1, 20, 1)        # Example values\n",
    "}\n",
    "\n",
    "# --- Initialization for Raw Results ---\n",
    "hdbscan_raw_results = [] # To store all RAW results for analysis\n",
    "\n",
    "# --- Perform Grid Search (Phase 1) ---\n",
    "total_iterations = len(hdbscan_params['min_cluster_size']) * len(hdbscan_params['min_samples'])\n",
    "with tqdm(total=total_iterations, desc=\"HDBSCAN Grid Search (Phase 1: Data Collection)\") as pbar:\n",
    "    for min_cluster_size in hdbscan_params['min_cluster_size']:\n",
    "        for min_samples in hdbscan_params['min_samples']:\n",
    "            hdbscan_model = hdbscan.HDBSCAN(\n",
    "                min_cluster_size=int(min_cluster_size),\n",
    "                min_samples=int(min_samples),\n",
    "                cluster_selection_method='eom', # Explicitly 'eom' for cluster_persistence_\n",
    "                allow_single_cluster=True        # Allows all non-noise points to be in a single cluster\n",
    "            )\n",
    "            labels = hdbscan_model.fit_predict(data_scaled)\n",
    "\n",
    "            # Determine the number of unique clusters (excluding noise)\n",
    "            unique_clusters = len(set(labels) - {-1})\n",
    "\n",
    "            # Consider only points assigned to a cluster (not noise)\n",
    "            clustered_points_indices = labels != -1\n",
    "\n",
    "            # Initialize metrics with default values for cases where they are not calculable\n",
    "            current_silhouette = -1.0 # Very poor if not calculable\n",
    "            current_negative_silhouette_ratio = 1.0 # All \"negative\" if not calculable\n",
    "            current_hdbscan_persistence = 0.0 # No persistence if not calculable\n",
    "\n",
    "            # Calculate metrics if sufficient clusters and points are available\n",
    "            if np.sum(clustered_points_indices) >= 2 and unique_clusters >= 2:\n",
    "                current_silhouette = silhouette_score(data_scaled[clustered_points_indices], labels[clustered_points_indices])\n",
    "                \n",
    "                silhouette_vals = silhouette_samples(data_scaled[clustered_points_indices], labels[clustered_points_indices])\n",
    "                current_negative_silhouette_ratio = np.sum(silhouette_vals < 0) / len(silhouette_vals)\n",
    "            \n",
    "            # Calculate cluster persistence (only if 'eom' and clusters exist)\n",
    "            if hdbscan_model.cluster_persistence_ is not None and len(hdbscan_model.cluster_persistence_) > 0:\n",
    "                current_hdbscan_persistence = np.mean(hdbscan_model.cluster_persistence_)\n",
    "            # If no clusters or only noise/a single cluster, hdbscan_persistence_ remains 0.0\n",
    "\n",
    "            # Noise statistics\n",
    "            noise_count = np.sum(labels == -1)\n",
    "            current_noise_ratio = noise_count / len(labels)\n",
    "\n",
    "            # Store results (Raw values, NO combined score in this phase)\n",
    "            hdbscan_raw_results.append({\n",
    "                'min_cluster_size': int(min_cluster_size),\n",
    "                'min_samples': int(min_samples),\n",
    "                'unique_clusters': unique_clusters,\n",
    "                'silhouette_avg': current_silhouette,\n",
    "                'negative_silhouette_ratio': current_negative_silhouette_ratio,\n",
    "                'noise_ratio': current_noise_ratio,\n",
    "                'hdbscan_persistence': current_hdbscan_persistence # Store raw value\n",
    "            })\n",
    "            \n",
    "            pbar.update(1) # Update progress bar\n",
    "\n",
    "# Convert the collected raw data into a DataFrame\n",
    "hdbscan_results_df = pd.DataFrame(hdbscan_raw_results)\n",
    "# Save the complete results\n",
    "hdbscan_results_df.to_pickle('data/processed_data/pkl/cluster_results/hdbscan_grid_search_results_tsne.pickle')\n",
    "print(\"\\nAll Grid Search results saved to 'data/processed_data/pkl/cluster_results/hdbscan_grid_search_results_tsne.pickle'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeba7b4-bf10-47e6-91b8-1ad134b14309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_combined_score_normalized(\n",
    "    silhouette_avg,\n",
    "    negative_silhouette_ratio,\n",
    "    noise_ratio,\n",
    "    hdbscan_persistence,\n",
    "    min_overall_persistence, # Global min value for normalization\n",
    "    max_overall_persistence, # Global max value for normalization\n",
    "    noise_threshold=0.10, # 10%\n",
    "    W1=1.0, # Weight for normalized average silhouette score (Maximize)\n",
    "    W2=0.5, # Weight for normalized negative silhouette ratio (Minimize)\n",
    "    W3=1.5, # Weight for normalized HDBSCAN persistence (Maximize)\n",
    "    W4=5.0  # High penalty for exceeding noise threshold\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates a combined score for optimizing HDBSCAN parameters after normalization.\n",
    "    Uses Min-Max normalization for hdbscan_persistence based on global min/max values.\n",
    "\n",
    "    Parameters:\n",
    "    - silhouette_avg (float): Average silhouette score.\n",
    "    - negative_silhouette_ratio (float): Ratio of negative silhouette scores.\n",
    "    - noise_ratio (float): Ratio of noise points.\n",
    "    - hdbscan_persistence (float): HDBSCAN persistence value.\n",
    "    - min_overall_persistence (float): Global minimum persistence value for normalization.\n",
    "    - max_overall_persistence (float): Global maximum persistence value for normalization.\n",
    "    - noise_threshold (float): Maximum acceptable noise ratio before applying a penalty.\n",
    "    - W1, W2, W3, W4 (float): Weights for the different metrics in the combined score.\n",
    "\n",
    "    Returns:\n",
    "    - float: The calculated combined score.\n",
    "    \"\"\"\n",
    "    # --- Normalize Metrics to the Range [0, 1] ---\n",
    "\n",
    "    # 1. silhouette_avg: Range [-1, 1] -> [0, 1]\n",
    "    # A score of -1 becomes 0, a score of 1 becomes 1.\n",
    "    normalized_silhouette_avg = (silhouette_avg + 1) / 2\n",
    "\n",
    "    # 2. negative_silhouette_ratio: Already in range [0, 1]\n",
    "    normalized_negative_silhouette_ratio = negative_silhouette_ratio\n",
    "\n",
    "    # 3. noise_ratio: Already in range [0, 1]\n",
    "    normalized_noise_ratio = noise_ratio\n",
    "\n",
    "    # 4. hdbscan_persistence: Min-Max Normalization\n",
    "    # Ensure division by zero is avoided if max and min are equal (e.g., all persistences are 0)\n",
    "    if max_overall_persistence > min_overall_persistence:\n",
    "        normalized_hdbscan_persistence = (hdbscan_persistence - min_overall_persistence) / \\\n",
    "                                         (max_overall_persistence - min_overall_persistence)\n",
    "    else: # All persistence values are equal (e.g., all 0) or only one value\n",
    "        normalized_hdbscan_persistence = 0.0 # No variation, thus no positive contribution from persistence\n",
    "\n",
    "    # --- Calculate Combined Score ---\n",
    "    # Maximize silhouette_avg and hdbscan_persistence (positive weights)\n",
    "    # Minimize negative_silhouette_ratio (negative weight)\n",
    "    combined_score = W1 * normalized_silhouette_avg\n",
    "    combined_score -= W2 * normalized_negative_silhouette_ratio\n",
    "    combined_score += W3 * normalized_hdbscan_persistence\n",
    "\n",
    "    # Noise ratio: High penalty if threshold is exceeded\n",
    "    if normalized_noise_ratio > noise_threshold:\n",
    "        # The penalty is proportional to how much the threshold is exceeded\n",
    "        combined_score -= W4 * (normalized_noise_ratio - noise_threshold) * 10\n",
    "\n",
    "    return combined_score\n",
    "\n",
    "def process_hdbscan_grid_search_results(hdbscan_results_df: pd.DataFrame, output_filename: str):\n",
    "    \"\"\"\n",
    "    Processes the results of an HDBSCAN grid search, calculates normalized combined scores,\n",
    "    identifies the best parameters, and saves the enhanced DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - hdbscan_results_df (pd.DataFrame): DataFrame containing the raw HDBSCAN grid search results.\n",
    "                                         Expected columns: 'silhouette_avg', 'negative_silhouette_ratio',\n",
    "                                         'noise_ratio', 'hdbscan_persistence', 'min_cluster_size',\n",
    "                                         'min_samples', 'unique_clusters'.\n",
    "    - output_filename (str): The full path including filename to save the processed DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting processing for: {output_filename}\")\n",
    "\n",
    "    # --- Step 1: Determine Global Min/Max Values for Persistence ---\n",
    "    # Filter persistence values that are not 0 to make the normalization range meaningful.\n",
    "    # If all persistence values are 0, we treat this specially.\n",
    "    valid_persistences = hdbscan_results_df['hdbscan_persistence'][hdbscan_results_df['hdbscan_persistence'] > 0]\n",
    "\n",
    "    if not valid_persistences.empty:\n",
    "        min_overall_persistence = valid_persistences.min()\n",
    "        max_overall_persistence = valid_persistences.max()\n",
    "    else: # All persistence values are 0 (or no valid ones found)\n",
    "        min_overall_persistence = 0.0\n",
    "        max_overall_persistence = 0.0\n",
    "\n",
    "    print(f\"Phase 2: Calculating combined scores with normalized persistence.\")\n",
    "    print(f\"Global Min Persistence (excluding 0): {min_overall_persistence:.4f}, Max Persistence: {max_overall_persistence:.4f}\")\n",
    "\n",
    "    # --- Step 2: Calculate Combined Scores and Find Best Score/Parameters ---\n",
    "    best_score = -np.inf # Start with a very small value, as we want to maximize\n",
    "    best_params = {}\n",
    "    best_metrics = {}\n",
    "\n",
    "    # Iterate over the collected results and calculate the combined score\n",
    "    # tqdm provides a progress bar for the loop\n",
    "    for index, row in tqdm(hdbscan_results_df.iterrows(), total=len(hdbscan_results_df), desc=\"Calculating final scores\"):\n",
    "        current_combined_score = calculate_combined_score_normalized(\n",
    "            silhouette_avg=row['silhouette_avg'],\n",
    "            negative_silhouette_ratio=row['negative_silhouette_ratio'],\n",
    "            noise_ratio=row['noise_ratio'],\n",
    "            hdbscan_persistence=row['hdbscan_persistence'],\n",
    "            min_overall_persistence=min_overall_persistence,\n",
    "            max_overall_persistence=max_overall_persistence\n",
    "        )\n",
    "        \n",
    "        # Add the combined score as a new column to the DataFrame\n",
    "        hdbscan_results_df.at[index, 'combined_score'] = current_combined_score\n",
    "\n",
    "        # Update best score and parameters if the current score is better\n",
    "        if current_combined_score > best_score:\n",
    "            best_score = current_combined_score\n",
    "            best_params = {\n",
    "                'min_cluster_size': row['min_cluster_size'],\n",
    "                'min_samples': row['min_samples']\n",
    "            }\n",
    "            best_metrics = {\n",
    "                'silhouette_avg': row['silhouette_avg'],\n",
    "                'negative_silhouette_ratio': row['negative_silhouette_ratio'],\n",
    "                'noise_ratio': row['noise_ratio'],\n",
    "                'hdbscan_persistence': row['hdbscan_persistence'],\n",
    "                'unique_clusters': row['unique_clusters']\n",
    "            }\n",
    "\n",
    "    # --- Display and Save Results ---\n",
    "    print(\"\\n--- Grid Search Processing Completed ---\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Combined Score (normalized): {best_score:.4f}\")\n",
    "    print(f\"Associated Metrics (Raw Values):\")\n",
    "    for metric, value in best_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "    # Save the complete results to the specified pickle file\n",
    "    hdbscan_results_df.to_pickle(output_filename)\n",
    "    print(f\"\\nAll Grid Search results saved to '{output_filename}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e3e4ba-5e05-467b-9987-ccadfb74a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_tsne_df = pd.read_pickle('data/processed_data/pkl/cluster_results/hdbscan_grid_search_results_tsne.pickle')\n",
    "hdbscan_df = pd.read_pickle('data/processed_data/pkl/cluster_results/hdbscan_grid_search_results.pickle')\n",
    "\n",
    "process_hdbscan_grid_search_results(\n",
    "     hdbscan_results_df=hdbscan_df.copy(), # Use .copy() to avoid modifying original DF\n",
    "     output_filename='data/processed_data/pkl/cluster_results/hdbscan_grid_search_results_normalized.pickle'\n",
    " )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\") # Separator for clarity\n",
    "\n",
    "process_hdbscan_grid_search_results(\n",
    "     hdbscan_results_df=hdbscan_tsne_df.copy(), # Use .copy() to avoid modifying original DF\n",
    "     output_filename='data/processed_data/pkl/cluster_results/hdbscan_grid_search_results_tsne_normalized.pickle'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f8175-131a-4a00-882d-365aab2ad86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdbscan = pickle.load(open('data/processed_data/pkl/cluster_results/hdbscan_grid_search_results_tsne_normalized.pickle', 'rb'))\n",
    "df_hdbscan = df_hdbscan.sort_values(by='combined_score', ascending=False)\n",
    "df_hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83f581-8390-4d7f-af48-0c9ac0e1bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdbscan['min_cluster_size'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9963a8a4-8da7-45d9-adb7-4219a2b30ee2",
   "metadata": {},
   "source": [
    "## DBScan (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc60ce0-e52b-41ec-8a02-2807073c774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dbscan_grid_search(data, eps_values, min_samples_values, output_filename='dbscan_grid_search_results.pickle'):\n",
    "    \"\"\"\n",
    "    Performs a grid search for DBSCAN parameters (eps and min_samples) on the given data.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): The input data for DBSCAN clustering (e.g., t-SNE reduced data or scaled full feature space).\n",
    "        eps_values (np.ndarray or list): An array or list of 'eps' values to test.\n",
    "        min_samples_values (range or list): A range or list of 'min_samples' values to test.\n",
    "        output_filename (str): The filename (including path) to save the results DataFrame as a pickle file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the raw results of the grid search, including\n",
    "                      'eps', 'min_samples', 'unique_clusters', 'silhouette_avg',\n",
    "                      'negative_silhouette_ratio', and 'noise_ratio'.\n",
    "    \"\"\"\n",
    "    dbscan_raw_results = []\n",
    "    total_iterations = len(eps_values) * len(min_samples_values)\n",
    "\n",
    "    with tqdm(total=total_iterations, desc=\"DBSCAN Grid Search (Phase 1: Datensammlung)\") as pbar:\n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                dbscan_model = DBSCAN(\n",
    "                    eps=float(eps),\n",
    "                    min_samples=int(min_samples)\n",
    "                )\n",
    "                labels = dbscan_model.fit_predict(data)\n",
    "\n",
    "                unique_clusters = len(set(labels) - {-1})\n",
    "                clustered_points_indices = labels != -1\n",
    "                \n",
    "                current_silhouette = -1.0\n",
    "                current_negative_silhouette_ratio = 1.0\n",
    "\n",
    "                if unique_clusters >= 2 and np.sum(clustered_points_indices) >= 2:\n",
    "                    current_silhouette = silhouette_score(data[clustered_points_indices], labels[clustered_points_indices])\n",
    "                    \n",
    "                    silhouette_vals = silhouette_samples(data[clustered_points_indices], labels[clustered_points_indices])\n",
    "                    current_negative_silhouette_ratio = np.sum(silhouette_vals < 0) / len(silhouette_vals)\n",
    "                \n",
    "                noise_count = np.sum(labels == -1)\n",
    "                current_noise_ratio = noise_count / len(labels)\n",
    "\n",
    "                dbscan_raw_results.append({\n",
    "                    'eps': float(eps),\n",
    "                    'min_samples': int(min_samples),\n",
    "                    'unique_clusters': unique_clusters,\n",
    "                    'silhouette_avg': current_silhouette,\n",
    "                    'negative_silhouette_ratio': current_negative_silhouette_ratio,\n",
    "                    'noise_ratio': current_noise_ratio\n",
    "                })\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "    dbscan_results_df = pd.DataFrame(dbscan_raw_results)\n",
    "    dbscan_results_df.to_pickle(output_filename)\n",
    "    print(f\"\\nAlle Grid Search Ergebnisse gespeichert unter '{output_filename}'\")\n",
    "    return dbscan_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e432e69-c9e9-4fc0-9318-e61769054e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced = X_tsne\n",
    "\n",
    "# 2. Define your DBSCAN parameter ranges\n",
    "eps_values_to_test = np.arange(0.5, 5.5, 0.1)\n",
    "min_samples_values_to_test = range(5, 50, 1)\n",
    "\n",
    "# 3. Define the output filename\n",
    "output_file = 'data/processed_data/pkl/cluster_results/dbscan_grid_search_results_tsne.pickle'\n",
    "\n",
    "# 4. Call the function\n",
    "results_df = run_dbscan_grid_search(\n",
    "    data=data_reduced,\n",
    "    eps_values=eps_values_to_test,\n",
    "    min_samples_values=min_samples_values_to_test,\n",
    "    output_filename=output_file\n",
    ")\n",
    "\n",
    "# You can now work with the 'results_df' DataFrame\n",
    "print(\"\\nFirst 5 rows of the results DataFrame:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94077bf-83e1-467d-8862-8681ea5c3645",
   "metadata": {},
   "source": [
    "## DBScan (full feature space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46191ad-2696-4d9c-bc0b-7520f65b00a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_dbscan_grid_search_full_space(df_scaled, eps_values, min_samples_values, output_filename='dbscan_grid_search_results_full_space.pickle'):\n",
    "    \"\"\"\n",
    "    Performs a grid search for DBSCAN parameters (eps and min_samples) on the full feature space data.\n",
    "\n",
    "    Args:\n",
    "        df_scaled (pd.DataFrame): The scaled DataFrame, where the first column is assumed to be an ID\n",
    "                                   and subsequent columns are features.\n",
    "        eps_values (np.ndarray or list): An array or list of 'eps' values to test.\n",
    "        min_samples_values (range or list): A range or list of 'min_samples' values to test.\n",
    "        output_filename (str): The filename (including path) to save the results DataFrame as a pickle file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the raw results of the grid search, including\n",
    "                      'eps', 'min_samples', 'unique_clusters', 'silhouette_avg',\n",
    "                      'negative_silhouette_ratio', and 'noise_ratio'.\n",
    "    \"\"\"\n",
    "    # Use .values for direct NumPy array input to DBSCAN, skipping the first column (e.g., ID).\n",
    "    data_full_space = df_scaled.iloc[:, 1:].values\n",
    "\n",
    "    dbscan_raw_results_full_space = []\n",
    "    total_iterations = len(eps_values) * len(min_samples_values)\n",
    "\n",
    "    with tqdm(total=total_iterations, desc=\"DBSCAN Grid Search (Phase 1: Data Collection - Full Feature Space)\") as pbar:\n",
    "        for eps in eps_values:\n",
    "            for min_samples in min_samples_values:\n",
    "                dbscan_model = DBSCAN(\n",
    "                    eps=float(eps),\n",
    "                    min_samples=int(min_samples)\n",
    "                )\n",
    "                labels = dbscan_model.fit_predict(data_full_space)\n",
    "\n",
    "                # Filter out noise points (-1 label) for metric calculations\n",
    "                clustered_points_indices = labels != -1\n",
    "                \n",
    "                # Check for valid clustering results for metric calculation\n",
    "                unique_clusters = len(set(labels) - {-1})\n",
    "                \n",
    "                # Initialize metrics with default poor scores if not calculable\n",
    "                current_silhouette = -1.0\n",
    "                current_negative_silhouette_ratio = 1.0\n",
    "\n",
    "                # Calculate metrics if there are at least 2 clusters and at least 2 clustered points\n",
    "                if unique_clusters >= 2 and np.sum(clustered_points_indices) >= 2:\n",
    "                    current_silhouette = silhouette_score(data_full_space[clustered_points_indices], labels[clustered_points_indices])\n",
    "                    \n",
    "                    silhouette_vals = silhouette_samples(data_full_space[clustered_points_indices], labels[clustered_points_indices])\n",
    "                    current_negative_silhouette_ratio = np.sum(silhouette_vals < 0) / len(silhouette_vals)\n",
    "                \n",
    "                # Calculate noise statistics (always applicable)\n",
    "                noise_count = np.sum(labels == -1)\n",
    "                current_noise_ratio = noise_count / len(labels)\n",
    "\n",
    "                # Store all raw results for later detailed analysis. Combined score calculated later.\n",
    "                dbscan_raw_results_full_space.append({\n",
    "                    'eps': float(eps),\n",
    "                    'min_samples': int(min_samples),\n",
    "                    'unique_clusters': unique_clusters, # Store raw value\n",
    "                    'silhouette_avg': current_silhouette,\n",
    "                    'negative_silhouette_ratio': current_negative_silhouette_ratio,\n",
    "                    'noise_ratio': current_noise_ratio\n",
    "                })\n",
    "                \n",
    "                pbar.update(1) # Update progress bar\n",
    "\n",
    "    # Convert the collected raw data into a DataFrame\n",
    "    dbscan_results_full_space_df = pd.DataFrame(dbscan_raw_results_full_space)\n",
    "    # Save the complete results\n",
    "    dbscan_results_full_space_df.to_pickle(output_filename)\n",
    "    print(f\"\\nAlle Grid Search Ergebnisse gespeichert unter '{output_filename}'\")\n",
    "    return dbscan_results_full_space_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691b324-8e44-4f09-9ebe-d43e8b15c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define your DBSCAN parameter ranges\n",
    "eps_values_to_test_full_space = np.arange(0.5, 5.0, 0.1)\n",
    "min_samples_values_to_test_full_space = range(5, 50, 1)\n",
    "\n",
    "# 3. Define the output filename\n",
    "output_file_full_space = 'data/processed_data/pkl/cluster_results/dbscan_grid_search_results_full_space.pickle'\n",
    "\n",
    "# 4. Call the function\n",
    "results_df_full_space = run_dbscan_grid_search_full_space(\n",
    "    df_scaled=df_scaled,\n",
    "    eps_values=eps_values_to_test_full_space,\n",
    "    min_samples_values=min_samples_values_to_test_full_space,\n",
    "    output_filename=output_file_full_space\n",
    ")\n",
    "\n",
    "# You can now work with the 'results_df_full_space' DataFrame\n",
    "print(\"\\nFirst 5 rows of the full feature space results DataFrame:\")\n",
    "print(results_df_full_space.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96424f1-ca20-425e-b920-4f4a2acdb1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funktion zur Berechnung des kombinierten Scores (basierend auf Code 2 / Englisch) ---\n",
    "def calculate_combined_score_dbscan_normalized_v2(\n",
    "    silhouette_avg,\n",
    "    negative_silhouette_ratio,\n",
    "    noise_ratio,\n",
    "    num_clusters,\n",
    "    min_overall_clusters, # Global min value for num_clusters normalization\n",
    "    max_overall_clusters, # Global max value for num_clusters normalization\n",
    "    noise_threshold=0.10, # 10% maximum allowable noise ratio\n",
    "    W1=1.0, # Weight for normalized average silhouette score (Maximize)\n",
    "    W2=0.5, # Weight for normalized negative silhouette ratio (Minimize)\n",
    "    W3_noise_penalty=5.0, # High penalty for exceeding noise threshold\n",
    "    W4_cluster_count_bonus=1.0, # Bonus for cluster count in desired range\n",
    "    target_min_clusters=2, # Lower bound of desired cluster range (e.g., at least 2 clusters)\n",
    "    target_max_clusters=10 # Upper bound of desired cluster range (e.g., max 10 clusters)\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates a combined score to optimize DBSCAN parameters after normalization.\n",
    "    Uses Min-Max normalization for num_clusters based on global min/max values.\n",
    "    Applies bonus/penalty based on a target range for the number of clusters.\n",
    "    This version applies a fixed bonus when num_clusters is in the target range.\n",
    "    \"\"\"\n",
    "    # --- Normalization of Metrics to the [0, 1] range ---\n",
    "\n",
    "    # 1. silhouette_avg: Range [-1, 1] -> [0, 1]\n",
    "    normalized_silhouette_avg = (silhouette_avg + 1) / 2\n",
    "\n",
    "    # 2. negative_silhouette_ratio: Already in [0, 1]\n",
    "    normalized_negative_silhouette_ratio = negative_silhouette_ratio\n",
    "\n",
    "    # 3. noise_ratio: Already in [0, 1]\n",
    "    normalized_noise_ratio = noise_ratio\n",
    "\n",
    "    # 4. num_clusters: Min-Max normalization based on global observed range\n",
    "    normalized_num_clusters = 0.0\n",
    "    if max_overall_clusters > min_overall_clusters:\n",
    "        # If there's a range, normalize it\n",
    "        normalized_num_clusters = (num_clusters - min_overall_clusters) / \\\n",
    "                                 (max_overall_clusters - min_overall_clusters)\n",
    "    # If max_overall_clusters == min_overall_clusters (e.g., all 0 or all 1 cluster),\n",
    "    # normalized_num_clusters remains 0.0, indicating no variability to contribute positively.\n",
    "\n",
    "    # --- Calculate the Combined Score ---\n",
    "    combined_score = W1 * normalized_silhouette_avg          # Maximize (good)\n",
    "    combined_score -= W2 * normalized_negative_silhouette_ratio # Minimize (bad)\n",
    "\n",
    "    # Noise ratio: High penalty if threshold is exceeded\n",
    "    if normalized_noise_ratio > noise_threshold:\n",
    "        combined_score -= W3_noise_penalty * (normalized_noise_ratio - noise_threshold) * 10\n",
    "\n",
    "    # Bonus/Penalty for the number of clusters based on desired range\n",
    "    if num_clusters >= target_min_clusters and num_clusters <= target_max_clusters:\n",
    "        combined_score += W4_cluster_count_bonus # Fixed bonus for being in range\n",
    "    else:\n",
    "        # Outside the target range: apply a penalty\n",
    "        if num_clusters < target_min_clusters:\n",
    "            combined_score -= W4_cluster_count_bonus * (target_min_clusters - num_clusters) # Penalize for being too few\n",
    "        elif num_clusters > target_max_clusters:\n",
    "            combined_score -= W4_cluster_count_bonus * (num_clusters - target_max_clusters) # Penalize for being too many\n",
    "\n",
    "    return combined_score\n",
    "\n",
    "# --- Hilfsfunktion zur Durchführung der Score-Berechnung für einen DataFrame ---\n",
    "def evaluate_dbscan_results(df, name, target_min_clusters, target_max_clusters):\n",
    "    \"\"\"\n",
    "    Evaluates DBSCAN results for a given DataFrame using the scoring function.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Evaluation for: {name} ---\")\n",
    "\n",
    "    # --- Step 2: Determine Global Min/Max for unique_clusters ---\n",
    "    # Filter out 0 cluster counts to get a more meaningful min/max for normalization of \"found\" clusters.\n",
    "    valid_unique_clusters = df['unique_clusters'][df['unique_clusters'] > 0]\n",
    "\n",
    "    if not valid_unique_clusters.empty:\n",
    "        min_overall_clusters = valid_unique_clusters.min()\n",
    "        max_overall_clusters = valid_unique_clusters.max()\n",
    "    else: # All cluster counts are 0 (or no valid ones found)\n",
    "        min_overall_clusters = 0\n",
    "        max_overall_clusters = 0\n",
    "\n",
    "    print(f\"Global Min Cluster Count (excluding 0): {min_overall_clusters}, Max Cluster Count: {max_overall_clusters}\")\n",
    "\n",
    "    # --- Initialize for Best Results ---\n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    best_metrics = {}\n",
    "    \n",
    "    # Ensure 'combined_score' column exists\n",
    "    df['combined_score'] = np.nan\n",
    "\n",
    "    # --- Step 3: Calculate Combined Scores and Find Best Parameters ---\n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=f\"Calculating Scores for {name}\"):\n",
    "        current_combined_score = calculate_combined_score_dbscan_normalized_v2(\n",
    "            silhouette_avg=row['silhouette_avg'],\n",
    "            negative_silhouette_ratio=row['negative_silhouette_ratio'],\n",
    "            noise_ratio=row['noise_ratio'],\n",
    "            num_clusters=row['unique_clusters'],\n",
    "            min_overall_clusters=min_overall_clusters,\n",
    "            max_overall_clusters=max_overall_clusters,\n",
    "            target_min_clusters=target_min_clusters,\n",
    "            target_max_clusters=target_max_clusters\n",
    "        )\n",
    "        \n",
    "        # Add the combined score as a new column to the DataFrame\n",
    "        df.at[index, 'combined_score'] = current_combined_score\n",
    "\n",
    "        # Update best score and parameters if current score is better\n",
    "        if current_combined_score > best_score:\n",
    "            best_score = current_combined_score\n",
    "            best_params = {\n",
    "                'eps': row['eps'],\n",
    "                'min_samples': row['min_samples']\n",
    "            }\n",
    "            best_metrics = {\n",
    "                'silhouette_avg': row['silhouette_avg'],\n",
    "                'negative_silhouette_ratio': row['negative_silhouette_ratio'],\n",
    "                'noise_ratio': row['noise_ratio'],\n",
    "                'unique_clusters': row['unique_clusters']\n",
    "            }\n",
    "\n",
    "    # --- Display Results ---\n",
    "    print(f\"\\n--- DBSCAN Grid Search for {name} Completed ---\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Combined Score (Normalized): {best_score:.4f}\")\n",
    "    print(f\"Corresponding Metrics (Raw Values):\")\n",
    "    for metric, value in best_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return df, best_params, best_metrics, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29bf7d6-15a0-4bb8-9173-69aabec7e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_tsne_df = pd.read_pickle('data/processed_data/pkl/cluster_results/dbscan_grid_search_results_tsne.pickle')\n",
    "dbscan_full_space_df = pd.read_pickle('data/processed_data/pkl/cluster_results/dbscan_grid_search_results_full_space.pickle')\n",
    "# Option 1: t-SNE Daten\n",
    "# Nehmen wir an, für t-SNE Daten wünschen wir 3-7 Cluster\n",
    "tsne_df_evaluated, tsne_best_params, tsne_best_metrics, tsne_best_score = evaluate_dbscan_results(\n",
    "    dbscan_tsne_df.copy(), # .copy() um sicherzustellen, dass der Original-DF nicht direkt modifiziert wird\n",
    "    \"t-SNE Data\",\n",
    "    target_min_clusters=2,\n",
    "    target_max_clusters=25\n",
    ")\n",
    "\n",
    "# Option 2: Full Feature Space Daten\n",
    "# Nehmen wir an, für Full Feature Space Daten wünschen wir 5-15 Cluster\n",
    "full_space_df_evaluated, full_space_best_params, full_space_best_metrics, full_space_best_score = evaluate_dbscan_results(\n",
    "    dbscan_full_space_df.copy(), # .copy() aus dem gleichen Grund\n",
    "    \"Full Feature Space Data\",\n",
    "    target_min_clusters=2,\n",
    "    target_max_clusters=25\n",
    ")\n",
    "\n",
    "# Optional: Anzeigen der DataFrames mit den hinzugefügten Scores\n",
    "print(\"\\n--- Evaluated t-SNE DataFrame (first 5 rows) ---\")\n",
    "display(tsne_df_evaluated.head())\n",
    "\n",
    "print(\"\\n--- Evaluated Full Feature Space DataFrame (first 5 rows) ---\")\n",
    "display(full_space_df_evaluated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3a3ff-9c07-4811-90ae-db0e7ae6e08b",
   "metadata": {},
   "source": [
    "## Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eabf02-bf9f-4ba4-9970-8c21fc738386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path for the seed\n",
    "SEED_FILE = \"data/processed_data/pkl/random_seed.pkl\"\n",
    "\n",
    "# Function to save the seed\n",
    "def save_random_seed(seed):\n",
    "    with open(SEED_FILE, 'wb') as f:\n",
    "        pickle.dump(seed, f)\n",
    "    print(f\"Random seed {seed} saved to {SEED_FILE}\")\n",
    "\n",
    "# Function to load the seed\n",
    "def load_random_seed():\n",
    "    if os.path.exists(SEED_FILE):\n",
    "        with open(SEED_FILE, 'rb') as f:\n",
    "            seed = pickle.load(f)\n",
    "        print(f\"Random seed {seed} loaded from {SEED_FILE}\")\n",
    "        return seed\n",
    "    else:\n",
    "        print(f\"No seed file found at {SEED_FILE}. Generating a new seed.\")\n",
    "        return None\n",
    "\n",
    "# Function to generate the color mapping\n",
    "def generate_color_mapping_hsv(labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    cluster_labels = unique_labels[unique_labels != -1]\n",
    "    num_clusters = len(cluster_labels)\n",
    "\n",
    "    all_colors = []\n",
    "    # Collect colors from 'tab10' colormap\n",
    "    for i in range(plt.cm.get_cmap('tab10').N):\n",
    "        all_colors.append(plt.cm.get_cmap('tab10')(i))\n",
    "    # Collect colors from 'tab20' colormap\n",
    "    for i in range(plt.cm.get_cmap('tab20').N):\n",
    "        all_colors.append(plt.cm.get_cmap('tab20')(i))\n",
    "\n",
    "    # Select enough colors for the clusters, up to the total available\n",
    "    selected_colors = all_colors[:min(num_clusters, len(all_colors))]\n",
    "\n",
    "    # --- IMPORTANT: Integrate seed logic here ---\n",
    "    loaded_seed = load_random_seed()\n",
    "    if loaded_seed is not None:\n",
    "        random.seed(loaded_seed)\n",
    "    else:\n",
    "        # Generate a new seed and save it if none was found\n",
    "        new_seed = random.randint(0, 2**32 - 1) # Random seed in the range 0 to 2^32 - 1\n",
    "        random.seed(new_seed)\n",
    "        save_random_seed(new_seed)\n",
    "    # --- End of seed logic ---\n",
    "\n",
    "    # Shuffle colors - this is now deterministic if the seed is loaded\n",
    "    random.shuffle(selected_colors)\n",
    "\n",
    "    custom_cmap = mcolors.ListedColormap(selected_colors)\n",
    "\n",
    "    color_dict = {}\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        rgba_color = custom_cmap(i % len(selected_colors))\n",
    "        # Convert RGBA to Hexadecimal color format\n",
    "        hex_color = \"#{:02x}{:02x}{:02x}\".format(\n",
    "            int(rgba_color[0] * 255),\n",
    "            int(rgba_color[1] * 255),\n",
    "            int(rgba_color[2] * 255)\n",
    "        )\n",
    "        color_dict[label] = hex_color\n",
    "\n",
    "    # Assign black color to noise points (-1 label)\n",
    "    if -1 in unique_labels:\n",
    "        color_dict[-1] = \"#000000\"\n",
    "\n",
    "    return color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87b80c-dca9-4158-b858-1b95c6b75a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = df_scaled.iloc[:, 1:-4] \n",
    "\n",
    "# ---------------------------\n",
    "# HDBSCAN – Full Feature Space\n",
    "# ---------------------------\n",
    "\n",
    "# Initialize and fit HDBSCAN model for the full feature space\n",
    "hdbscan_full_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=34,\n",
    "    min_samples=1\n",
    ")\n",
    "hdbscan_labels_full = hdbscan_full_model.fit_predict(full_data)\n",
    "\n",
    "# Create a DataFrame for HDBSCAN results in full feature space\n",
    "df_hdbscan_full = pd.DataFrame(full_data).copy()\n",
    "df_hdbscan_full['cluster_id'] = hdbscan_labels_full\n",
    "hdbscan_full_counts = Counter(hdbscan_labels_full) # Count occurrences of each cluster label\n",
    "\n",
    "# Assign colors based on cluster IDs for HDBSCAN in full feature space\n",
    "color_dict_hdbscan_full = generate_color_mapping_hsv(hdbscan_labels_full)\n",
    "df_hdbscan_full['color'] = df_hdbscan_full['cluster_id'].map(color_dict_hdbscan_full)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# DBSCAN – Full Feature Space\n",
    "# ---------------------------\n",
    "\n",
    "# Initialize and fit DBSCAN model for the full feature space\n",
    "dbscan_full_model = DBSCAN(\n",
    "    eps=4.8,\n",
    "    min_samples=11\n",
    ")\n",
    "dbscan_labels_full = dbscan_full_model.fit_predict(full_data)\n",
    "\n",
    "# Create a DataFrame for DBSCAN results in full feature space\n",
    "df_dbscan_full = pd.DataFrame(full_data).copy()\n",
    "df_dbscan_full['cluster_id'] = dbscan_labels_full\n",
    "dbscan_full_counts = Counter(dbscan_labels_full) # Count occurrences of each cluster label\n",
    "\n",
    "# Assign colors based on cluster IDs for DBSCAN in full feature space\n",
    "color_dict_dbscan_full = generate_color_mapping_hsv(dbscan_labels_full)\n",
    "df_dbscan_full['color'] = df_dbscan_full['cluster_id'].map(color_dict_dbscan_full)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# DBSCAN – t-SNE Space\n",
    "# ---------------------------\n",
    "\n",
    "# Assuming X_tsne is already defined and loaded (e.g., from t-SNE reduction)\n",
    "\n",
    "# Initialize and fit DBSCAN model for the t-SNE reduced space\n",
    "dbscan_tsne_model = DBSCAN(\n",
    "    eps=5.1,\n",
    "    min_samples=42\n",
    ")\n",
    "dbscan_labels_tsne = dbscan_tsne_model.fit_predict(X_tsne)\n",
    "dbscan_tsne_counts = Counter(dbscan_labels_tsne) # Count occurrences of each cluster label\n",
    "\n",
    "# Create DataFrame for t-SNE results\n",
    "# Note: X_tsne is a NumPy array, so assign column names\n",
    "df_dbscan_tsne = pd.DataFrame(X_tsne, columns=['t-SNE Dim 1', 't-SNE Dim 2'])\n",
    "df_dbscan_tsne['cluster_id'] = dbscan_labels_tsne\n",
    "\n",
    "# Assign colors based on cluster IDs for DBSCAN in t-SNE space\n",
    "color_dict_dbscan_tsne = generate_color_mapping_hsv(dbscan_labels_tsne)\n",
    "df_dbscan_tsne['color'] = df_dbscan_tsne['cluster_id'].map(color_dict_dbscan_tsne)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# HDBSCAN – t-SNE Space\n",
    "# ---------------------------\n",
    "\n",
    "# Initialize and fit HDBSCAN model for the t-SNE reduced space\n",
    "hdbscan_tsne_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=45,\n",
    "    min_samples=3\n",
    ")\n",
    "hdbscan_labels_tsne = hdbscan_tsne_model.fit_predict(X_tsne)\n",
    "hdbscan_tsne_counts = Counter(hdbscan_labels_tsne) # Count occurrences of each cluster label\n",
    "\n",
    "# Create DataFrame for t-SNE results\n",
    "df_hdbscan_tsne = pd.DataFrame(X_tsne, columns=['t-SNE Dim 1', 't-SNE Dim 2'])\n",
    "df_hdbscan_tsne['cluster_id'] = hdbscan_labels_tsne\n",
    "\n",
    "# Assign colors based on cluster IDs for HDBSCAN in t-SNE space\n",
    "color_dict_hdbscan_tsne = generate_color_mapping_hsv(hdbscan_labels_tsne)\n",
    "df_hdbscan_tsne['color'] = df_hdbscan_tsne['cluster_id'].map(color_dict_hdbscan_tsne)\n",
    "\n",
    "print(\"\\n--- Clustering completed. DataFrames with cluster IDs and colors created. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724a29d-fdcd-4ad4-a793-792e4355609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster plotting function with annotation and legend\n",
    "def plot_clusters(ax, embedding, cluster_labels, counts, col_dict, method_label, xlim=None):\n",
    "    \"\"\"\n",
    "    Plots cluster results on an axes object.\n",
    "\n",
    "    Args:\n",
    "        ax (matplotlib.axes.Axes): The axes object to plot on.\n",
    "        embedding (np.array): The 2D embedding (e.g., t-SNE results).\n",
    "        cluster_labels (np.array): The assigned cluster labels for each point.\n",
    "        counts (collections.Counter): Counts of points per label.\n",
    "        col_dict (dict): Dictionary for color mapping for each label.\n",
    "        method_label (str): Label for the method used (top right of the plot).\n",
    "        xlim (tuple, optional): Limits for the X-axis. Default is None.\n",
    "    \"\"\"\n",
    "    for label in np.unique(cluster_labels):\n",
    "        idx = (cluster_labels == label)\n",
    "        \n",
    "        # Set color and alpha value\n",
    "        if label == -1:\n",
    "            plot_color = 'blue'  # Noise points in blue\n",
    "            plot_alpha = 1.0     # No alpha for noise points\n",
    "            label_name = f'Noise ({counts[label]})' # Legend label for noise points\n",
    "        else:\n",
    "            plot_color = col_dict[label] # Cluster color from dictionary\n",
    "            plot_alpha = 0.6             # Alpha for cluster points\n",
    "            label_name = None # No legend label for clusters\n",
    "            \n",
    "        ax.scatter(embedding[idx, 0], embedding[idx, 1],\n",
    "                   color=plot_color, label=label_name, alpha=plot_alpha, s=1)\n",
    "    \n",
    "    # Adjust X-axis limit if provided\n",
    "    if xlim:\n",
    "        ax.set_xlim(xlim)\n",
    "    \n",
    "    # Show noise legend only if noise is present\n",
    "    if -1 in counts:\n",
    "        ax.legend(loc='lower left', fontsize=8) # Smaller font for legend\n",
    "\n",
    "    # Method label in the top right corner\n",
    "    ax.text(0.95, 0.90, method_label, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='bottom', horizontalalignment='right',\n",
    "            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.3'))\n",
    "\n",
    "    # Circle annotation for cluster IDs\n",
    "    x_range = ax.get_xlim()[1] - ax.get_xlim()[0]\n",
    "    radius = 0.029 * x_range # Relative radius for the circles\n",
    "    y_offset = radius * -1.5 # Y-offset for positioning the circle\n",
    "    for label in np.unique(cluster_labels):\n",
    "        if label == -1: # Do not annotate noise points\n",
    "            continue\n",
    "        idx = (cluster_labels == label)\n",
    "        # Center of the cluster for annotation\n",
    "        x_center = np.mean(embedding[idx, 0])\n",
    "        y_center = np.mean(embedding[idx, 1])\n",
    "        circle_center = (x_center, y_center + y_offset)\n",
    "        circle = plt.Circle(circle_center, radius, color='white', ec='black', zorder=10)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(circle_center[0], circle_center[1], str(label),\n",
    "                fontsize=10, color='black',\n",
    "                horizontalalignment='center', verticalalignment='center', zorder=11)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Create 2x2 Subplot Layout\n",
    "# ---------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8)) # Slightly larger figure for better visibility\n",
    "\n",
    "# Plot 1 (Top Left): DBSCAN – Full Feature Space\n",
    "# Labels were generated on the full feature space, but visualized here on the t-SNE embedding.\n",
    "plot_clusters(axes[0, 0], X_tsne, dbscan_labels_full, dbscan_full_counts, color_dict_dbscan_full,\n",
    "              'DBSCAN (full features)')\n",
    "\n",
    "# Plot 2 (Top Right): DBSCAN – t-SNE Space\n",
    "plot_clusters(axes[0, 1], X_tsne, dbscan_labels_tsne, dbscan_tsne_counts, color_dict_dbscan_tsne,\n",
    "              'DBSCAN (t-SNE space)')\n",
    "\n",
    "# Plot 3 (Bottom Left): HDBSCAN – Full Feature Space\n",
    "# Labels were generated on the full feature space, but visualized here on the t-SNE embedding.\n",
    "plot_clusters(axes[1, 0], X_tsne, hdbscan_labels_full, hdbscan_full_counts, color_dict_hdbscan_full,\n",
    "              'HDBSCAN (full features)')\n",
    "\n",
    "# Plot 4 (Bottom Right): HDBSCAN – t-SNE Space\n",
    "plot_clusters(axes[1, 1], X_tsne, hdbscan_labels_tsne, hdbscan_tsne_counts, color_dict_hdbscan_tsne,\n",
    "              'HDBSCAN (t-SNE space)')\n",
    "\n",
    "\n",
    "# Remove axis labels for a cleaner look\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "# Adjust spacing\n",
    "plt.subplots_adjust(hspace=0.0, wspace=0.0)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('cluster_comparison_plot.png', dpi=600, bbox_inches='tight')\n",
    "# plt.savefig('cluster_comparison_plot.svg', format='svg', bbox_inches='tight') # Uncomment if SVG is desired\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df541fa9-8aba-4f79-bd33-c02da40b4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = df_scaled.iloc[:, 1:] \n",
    "\n",
    "\n",
    "# HDBSCAN – Full Feature Space Model\n",
    "hdbscan_full_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=34,\n",
    "    min_samples=1\n",
    ")\n",
    "# Re-fit to ensure we have the model object with persistence attribute\n",
    "hdbscan_labels_full = hdbscan_full_model.fit_predict(full_data)\n",
    "\n",
    "# DBSCAN – Full Feature Space Model (no persistence needed for DBSCAN)\n",
    "dbscan_full_model = DBSCAN(\n",
    "    eps=4.8,\n",
    "    min_samples=11\n",
    ")\n",
    "dbscan_labels_full = dbscan_full_model.fit_predict(full_data)\n",
    "\n",
    "\n",
    "# HDBSCAN – t-SNE Space Model\n",
    "hdbscan_tsne_model = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=45,\n",
    "    min_samples=3\n",
    ")\n",
    "hdbscan_labels_tsne = hdbscan_tsne_model.fit_predict(X_tsne)\n",
    "\n",
    "# DBSCAN – t-SNE Space Model\n",
    "dbscan_tsne_model = DBSCAN(\n",
    "    eps=5.1,\n",
    "    min_samples=42\n",
    ")\n",
    "dbscan_labels_tsne = dbscan_tsne_model.fit_predict(X_tsne)\n",
    "\n",
    "\n",
    "# --- Data for t-SNE Space ---\n",
    "data_tsne_space = X_tsne\n",
    "\n",
    "# --- Function to calculate clustering metrics for a clustering result ---\n",
    "def calculate_clustering_metrics(data_embedding, labels, setup_name, model=None):\n",
    "    \"\"\"\n",
    "    Calculates the defined clustering metrics for a given set of labels and data.\n",
    "\n",
    "    Args:\n",
    "        data_embedding (np.array or pd.DataFrame): The data (features) on which clustering is based.\n",
    "        labels (np.array): The labels assigned by the clustering method.\n",
    "        setup_name (str): A descriptive name for this clustering setup.\n",
    "        model (object, optional): The trained clustering model (especially for HDBSCAN persistence).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    results = {'Setup': setup_name}\n",
    "\n",
    "    # 1. Number of Clusters (excluding noise)\n",
    "    unique_clusters = len(set(labels) - {-1})\n",
    "    results['Number of Clusters'] = unique_clusters\n",
    "\n",
    "    # 2. Percentage of Noise\n",
    "    noise_count = np.sum(labels == -1)\n",
    "    total_points = len(labels)\n",
    "    results['Noise Ratio (%)'] = (noise_count / total_points) * 100\n",
    "\n",
    "    # Consider only points assigned to a cluster (not noise)\n",
    "    clustered_points_indices = labels != -1\n",
    "    \n",
    "    # Check if enough points are available for silhouette and density-based metrics\n",
    "    # (at least 2 clusters OR at least 2 points in clusters)\n",
    "    if unique_clusters < 2 or np.sum(clustered_points_indices) < 2:\n",
    "        results['Average Silhouette Score'] = np.nan\n",
    "        results['Negative Silhouette Ratio (%)'] = np.nan\n",
    "        results['Davies-Bouldin Index'] = np.nan\n",
    "        results['Calinski-Harabasz Index'] = np.nan\n",
    "    else:\n",
    "        # Filter data and labels to clustered points\n",
    "        filtered_data = data_embedding[clustered_points_indices]\n",
    "        filtered_labels = labels[clustered_points_indices]\n",
    "\n",
    "        # 3. Average Silhouette Score\n",
    "        results['Average Silhouette Score'] = silhouette_score(filtered_data, filtered_labels)\n",
    "\n",
    "        # 4. Percentage of Negative Silhouette Values\n",
    "        silhouette_vals = silhouette_samples(filtered_data, filtered_labels)\n",
    "        results['Negative Silhouette Ratio (%)'] = np.sum(silhouette_vals < 0) / len(silhouette_vals) * 100\n",
    "        \n",
    "        # 5. Davies-Bouldin Index\n",
    "        results['Davies-Bouldin Index'] = davies_bouldin_score(filtered_data, filtered_labels)\n",
    "        \n",
    "        # 6. Calinski-Harabasz Index\n",
    "        results['Calinski-Harabasz Index'] = calinski_harabasz_score(filtered_data, filtered_labels)\n",
    "    \n",
    "    # 7. HDBSCAN Persistence (if model available and HDBSCAN)\n",
    "    if model and hasattr(model, 'cluster_persistence_') and model.cluster_persistence_ is not None and len(model.cluster_persistence_) > 0:\n",
    "        results['Average HDBSCAN Persistence'] = np.mean(model.cluster_persistence_)\n",
    "    else:\n",
    "        results['Average HDBSCAN Persistence'] = np.nan # Not applicable or not available\n",
    "    \n",
    "    return results\n",
    "\n",
    "# List to store results of all four scenarios\n",
    "all_comparison_metrics = []\n",
    "\n",
    "print(\"Calculating metrics for clustering setups...\")\n",
    "\n",
    "# --- Metrics for HDBSCAN – Full Feature Space ---\n",
    "metrics_hdbscan_full_features = calculate_clustering_metrics(\n",
    "    data_embedding=full_data,\n",
    "    labels=hdbscan_labels_full,\n",
    "    setup_name=\"HDBSCAN (Full Features)\",\n",
    "    model=hdbscan_full_model # Pass the model for persistence\n",
    ")\n",
    "all_comparison_metrics.append(metrics_hdbscan_full_features)\n",
    "\n",
    "# --- Metrics for DBSCAN – Full Feature Space ---\n",
    "metrics_dbscan_full_features = calculate_clustering_metrics(\n",
    "    data_embedding=full_data,\n",
    "    labels=dbscan_labels_full,\n",
    "    setup_name=\"DBSCAN (Full Features)\",\n",
    "    model=None # No HDBSCAN model for persistence\n",
    ")\n",
    "all_comparison_metrics.append(metrics_dbscan_full_features)\n",
    "\n",
    "# --- Metrics for HDBSCAN – t-SNE Space ---\n",
    "metrics_hdbscan_tsne_space = calculate_clustering_metrics(\n",
    "    data_embedding=data_tsne_space,\n",
    "    labels=hdbscan_labels_tsne,\n",
    "    setup_name=\"HDBSCAN (t-SNE Space)\",\n",
    "    model=hdbscan_tsne_model # Pass the model for persistence\n",
    ")\n",
    "all_comparison_metrics.append(metrics_hdbscan_tsne_space)\n",
    "\n",
    "# --- Metrics for DBSCAN – t-SNE Space ---\n",
    "metrics_dbscan_tsne_space = calculate_clustering_metrics(\n",
    "    data_embedding=data_tsne_space,\n",
    "    labels=dbscan_labels_tsne,\n",
    "    setup_name=\"DBSCAN (t-SNE Space)\",\n",
    "    model=None # No HDBSCAN model for persistence\n",
    ")\n",
    "all_comparison_metrics.append(metrics_dbscan_tsne_space)\n",
    "\n",
    "\n",
    "# --- Create the comparison table ---\n",
    "comparison_df = pd.DataFrame(all_comparison_metrics)\n",
    "\n",
    "# --- Column order for better readability ---\n",
    "desired_columns = [\n",
    "    'Setup',\n",
    "    'Number of Clusters',\n",
    "    'Noise Ratio (%)',\n",
    "    'Average Silhouette Score',\n",
    "    'Negative Silhouette Ratio (%)',\n",
    "    'Davies-Bouldin Index',\n",
    "    'Calinski-Harabasz Index',\n",
    "    'Average HDBSCAN Persistence'\n",
    "]\n",
    "comparison_df = comparison_df[desired_columns]\n",
    "\n",
    "\n",
    "# --- Formatting for better readability ---\n",
    "# For Silhouette Score, Davies-Bouldin, Calinski-Harabasz, and Persistence\n",
    "for col in ['Average Silhouette Score', 'Davies-Bouldin Index', 'Calinski-Harabasz Index', 'Average HDBSCAN Persistence']:\n",
    "    comparison_df[col] = comparison_df[col].map(\n",
    "        lambda x: f'{x:.4f}' if pd.notna(x) else 'N/A'\n",
    "    )\n",
    "# For percentage values\n",
    "for col in ['Noise Ratio (%)', 'Negative Silhouette Ratio (%)']:\n",
    "    comparison_df[col] = comparison_df[col].map(\n",
    "        lambda x: f'{x:.2f}%' if pd.notna(x) else 'N/A'\n",
    "    )\n",
    "\n",
    "# --- Display the table ---\n",
    "print(\"\\n--- Quantitative Comparison of Clustering Results ---\")\n",
    "print(comparison_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ada1e8-cc2f-4d71-95a9-9c43ddfab68f",
   "metadata": {},
   "source": [
    "## Pairwise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78a576e-730e-486a-a760-773027aa16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Optional: Pairwise comparison of clusterings (ARI and NMI)\n",
    "# -------------------------------\n",
    "# These are external metrics comparing label similarity (even without ground truth)\n",
    "\n",
    "pairwise_metrics = pd.DataFrame({\n",
    "    'HDBSCAN (full feature set) vs HDBSCAN (t-SNE)': [\n",
    "        metrics.adjusted_rand_score(hdbscan_labels_full, hdbscan_labels_tsne),\n",
    "        metrics.normalized_mutual_info_score(hdbscan_labels_full, hdbscan_labels_tsne)\n",
    "    ],\n",
    "    'HDBSCAN (full feature set) vs DBSCAN (t-SNE)': [\n",
    "        metrics.adjusted_rand_score(hdbscan_labels_full, dbscan_labels_tsne),\n",
    "        metrics.normalized_mutual_info_score(hdbscan_labels_full, dbscan_labels_tsne)\n",
    "    ],\n",
    "    'HDBSCAN (t-SNE) vs DBSCAN (t-SNE)': [\n",
    "        metrics.adjusted_rand_score(hdbscan_labels_tsne, dbscan_labels_tsne),\n",
    "        metrics.normalized_mutual_info_score(hdbscan_labels_tsne, dbscan_labels_tsne)\n",
    "    ]\n",
    "}, index=['ARI', 'NMI'])\n",
    "\n",
    "print(\"\\nPairwise comparison metrics between methods:\")\n",
    "display(pairwise_metrics.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e2bf42-7525-423b-8fd9-abfca7d5b29a",
   "metadata": {},
   "source": [
    "# Vizualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace0735-40ff-406e-800d-fd78da574b4a",
   "metadata": {},
   "source": [
    "## t-SNE with heatmap for every single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99d4cf-e9f2-45ac-b4cf-d9bb82c90b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df_original_features, df_scaled_features, and X_tsne are already defined.\n",
    "\n",
    "# Create the feature ID dictionary\n",
    "feature_id_dict = {feature: \"F\" + str(i + 1) for i, feature in enumerate(df_original_features.columns)}\n",
    "df_vis = df_scaled_features.copy()\n",
    "\n",
    "# Iterate over the columns to create scatter plots\n",
    "for col in df_scaled_features:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
    "                          c=df_scaled_features[col],\n",
    "                          cmap='viridis',\n",
    "                          alpha=0.5,\n",
    "                          s=5)\n",
    "\n",
    "    # Add color bar legend\n",
    "    cbar = plt.colorbar(scatter, label=col)\n",
    "    label_text = feature_id_dict[col]\n",
    "\n",
    "    # Place the label as a textbox in the bottom right\n",
    "    plt.text(0.95, 0.05, label_text,\n",
    "             transform=plt.gca().transAxes,\n",
    "             horizontalalignment='right',\n",
    "             verticalalignment='bottom',\n",
    "             fontsize=20,\n",
    "             bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b75ef95-1346-4ff6-8fcc-fa9d916f0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_tsne, df_scaled_features, and feature_id_dict are already defined.\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
    "axs = axs.flatten()  # Allows easy access by index\n",
    "\n",
    "features_to_plot = ['total_emp_occu_51-4033', 'total_emp_naics_3335', 'total_emp_naics_3364', 'total_emp_naics_3366']\n",
    "\n",
    "# Common scaling for all four plots\n",
    "all_data_common = np.concatenate([\n",
    "    df_scaled_features[col].values for col in features_to_plot\n",
    "])\n",
    "vmin_common, vmax_common = all_data_common.min(), all_data_common.max()\n",
    "\n",
    "for i, col in enumerate(features_to_plot):\n",
    "    ax = axs[i]\n",
    "\n",
    "    data = df_scaled_features[col]\n",
    "    label_text = feature_id_dict[col]\n",
    "\n",
    "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
    "                         c=data,\n",
    "                         cmap='viridis',\n",
    "                         alpha=0.5,\n",
    "                         s=2,\n",
    "                         vmin=vmin_common,\n",
    "                         vmax=vmax_common)\n",
    "\n",
    "    # Place the textbox with the label in the bottom right of the plot\n",
    "    ax.text(0.95, 0.05, label_text,\n",
    "            transform=ax.transAxes,\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            fontsize=15,\n",
    "            bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "\n",
    "    # Remove axes and axis labels\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "# Common colorbar for all four plots\n",
    "cbar_ax = fig.add_axes([0.15, 0.05, 0.7, 0.02])  # [left, bottom, width, height]\n",
    "cbar = fig.colorbar(axs[0].collections[0], cax=cbar_ax, orientation='horizontal')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.1, 1, 1])\n",
    "plt.subplots_adjust(hspace=0.0, wspace=0.0)  # Prevents overlap with the colorbar\n",
    "plt.savefig('tsne_feature_values.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592efb4-836a-4f63-adc3-a95c85081ff8",
   "metadata": {},
   "source": [
    "## Cluster on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c957ab-5c73-47e8-83b9-8a3bc5064168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled['cluster_id'] = df_hdbscan_tsne['cluster_id']\n",
    "df_scaled['color'] = df_hdbscan_tsne['color']\n",
    "\n",
    "numeric_columns = df_scaled.columns[1:-3].tolist()\n",
    "# Calculation of means for numeric columns per cluster\n",
    "mean_values = df_scaled.groupby('cluster_id')[numeric_columns].mean()\n",
    "\n",
    "# Calculation of overall_mean as the average across all numeric columns\n",
    "mean_values['overall_mean'] = mean_values.mean(axis=1)\n",
    "\n",
    "# Sorting by overall_mean in descending order\n",
    "mean_values = mean_values.sort_values(by='overall_mean', ascending=False)\n",
    "\n",
    "# Adding a 'rank' column based on overall_mean\n",
    "mean_values['rank'] = mean_values['overall_mean'].rank(method='dense', ascending=False).astype(int)\n",
    "mean_values = mean_values.reset_index()\n",
    "mean_values.head()\n",
    "df_scaled = df_scaled.merge(mean_values[['rank', 'cluster_id']], on='cluster_id', how='left')\n",
    "df_scaled['overall_mean'] = df_scaled.iloc[:,1:-3].mean(axis=1)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7f17f-5def-46c1-8c5f-c8f18c84ed06",
   "metadata": {},
   "source": [
    "### one map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb6b07-1dd7-4a80-80e9-6c23ac9478e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Counties and States for Alaska (STATEFP=02)\n",
    "filtered_county_shape_north_america_merged = filtered_county_shape_north_america.merge(df_scaled[['FIPS', 'cluster_id','color','rank','overall_mean']], on='FIPS', how='left')\n",
    "filtered_county_shape_alaska_merged = filtered_county_shape_alaska.merge(df_scaled[['FIPS', 'cluster_id','color','rank','overall_mean']], on='FIPS', how='left')\n",
    "\n",
    "alaska_state = filtered_state_shape_north_america[filtered_state_shape_north_america['STATEFP'] == '02']\n",
    "alaska_counties = filtered_county_shape_north_america_merged[filtered_county_shape_north_america_merged['STATEFP'] == '02']\n",
    "\n",
    "# Filter remaining data\n",
    "remaining_states = filtered_state_shape_north_america[filtered_state_shape_north_america['STATEFP'] != '02']\n",
    "remaining_counties = filtered_county_shape_north_america_merged[filtered_county_shape_north_america_merged['STATEFP'] != '02']\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Plot remaining counties and states\n",
    "remaining_counties.plot(\n",
    "    facecolor=remaining_counties['color'],\n",
    "    linewidth=0.4,\n",
    "    ax=ax,\n",
    "    edgecolor='grey',\n",
    "    legend=False\n",
    ")\n",
    "remaining_states.plot(ax=ax, color='none', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Add a separate inset for Alaska\n",
    "ak_ax = fig.add_axes([0.62, 0.255, 0.275, 0.275]) # Position and size of the Alaska inset\n",
    "\n",
    "# Optionally clip Alaska to a polygon if needed\n",
    "polygon = Polygon([(-180, 50), (-180, 72), (-140, 72), (-140, 50)]) # Define bounding polygon\n",
    "alaska_state = alaska_state.clip(polygon) # Clip Alaska state shapes\n",
    "alaska_counties = alaska_counties.clip(polygon) # Clip Alaska counties\n",
    "\n",
    "# Plot Alaska in the inset\n",
    "alaska_counties.plot(\n",
    "    facecolor=alaska_counties['color'],\n",
    "    linewidth=0.4,\n",
    "    ax=ak_ax,\n",
    "    edgecolor='grey',\n",
    "    legend=False\n",
    ")\n",
    "alaska_state.plot(ax=ak_ax, color='none', edgecolor='black', linewidth=0.8)\n",
    "\n",
    "# Customize Alaska inset\n",
    "ak_ax.axis('off') # Turn off the axes\n",
    "ax.axis('off') # Turn off the axes\n",
    "\n",
    "# Add the legend for clusters to the main plot\n",
    "unique_clusters = sorted(filtered_county_shape_north_america_merged['cluster_id'].unique())\n",
    "cluster_counts = filtered_county_shape_north_america_merged['cluster_id'].value_counts()\n",
    "legend_elements = []\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    if cluster == -1:\n",
    "        count = cluster_counts.get(-1, 0)\n",
    "        legend_elements.append(Patch(facecolor='blue', label=f'noise ({count})'))\n",
    "    else:\n",
    "        color = filtered_county_shape_north_america_merged.loc[filtered_county_shape_north_america_merged['cluster_id'] == cluster, 'color'].values[0]\n",
    "        count = cluster_counts.get(cluster, 0)\n",
    "        legend_elements.append(Patch(facecolor=color, label=f'{cluster} ({count})'))\n",
    "\n",
    "ax.legend(handles=legend_elements, title='Cluster ID', loc='lower left',\n",
    "    title_fontsize=10, # Set the font size of the legend title\n",
    "    fontsize=8 )\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5741f-47d8-4f63-887e-2b00b3a16862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cluster data with county shapes\n",
    "filtered_county_shape_north_america_merged = filtered_county_shape_north_america.merge(df_scaled[['FIPS', 'cluster_id','color','rank','overall_mean']], on='FIPS', how='left')\n",
    "filtered_county_shape_alaska_merged = filtered_county_shape_alaska.merge(df_scaled[['FIPS', 'cluster_id','color','rank','overall_mean']], on='FIPS', how='left')\n",
    "\n",
    "\n",
    "filtered_counties = filtered_county_shape_north_america_merged[\n",
    "    (filtered_county_shape_north_america_merged['rank'] <= 5) |\n",
    "    (filtered_county_shape_north_america_merged['cluster_id'] == -1)\n",
    "]\n",
    "top_rank = filtered_counties[filtered_counties['rank'] <= 5]\n",
    "\n",
    "# Filter counties from cluster -1, sort them by overall_mean in descending order, and select the top 20\n",
    "cluster_minus_one_top20 = (\n",
    "    filtered_counties[filtered_counties['cluster_id'] == -1]\n",
    "    .sort_values(by='overall_mean', ascending=False)\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "# Combine both sub-dataframes and remove any duplicates\n",
    "filtered_counties = pd.concat([top_rank, cluster_minus_one_top20]).drop_duplicates()\n",
    "\n",
    "\n",
    "# Original data (for outlines/contours)\n",
    "all_counties = filtered_county_shape_north_america_merged.copy()\n",
    "\n",
    "# Split into Alaska and Rest\n",
    "# For Alaska:\n",
    "alaska_state = filtered_state_shape_north_america[\n",
    "    filtered_state_shape_north_america['STATEFP'] == '02'\n",
    "]\n",
    "alaska_counties_fill = filtered_counties[\n",
    "    filtered_counties['STATEFP'] == '02'\n",
    "]\n",
    "alaska_counties_all = all_counties[\n",
    "    all_counties['STATEFP'] == '02'\n",
    "]\n",
    "\n",
    "# For remaining states:\n",
    "remaining_states = filtered_state_shape_north_america[\n",
    "    filtered_state_shape_north_america['STATEFP'] != '02'\n",
    "]\n",
    "remaining_counties_fill = filtered_counties[\n",
    "    filtered_counties['STATEFP'] != '02'\n",
    "]\n",
    "remaining_counties_all = all_counties[\n",
    "    all_counties['STATEFP'] != '02'\n",
    "]\n",
    "\n",
    "# 2. Create the main plot\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "# Plot non-noise counties with their assigned colors\n",
    "remaining_counties_non_noise = remaining_counties_fill[remaining_counties_fill['cluster_id'] != -1]\n",
    "if not remaining_counties_non_noise.empty:\n",
    "    remaining_counties_non_noise.plot(\n",
    "        facecolor=remaining_counties_non_noise['color'],\n",
    "        linewidth=0.2,\n",
    "        ax=ax,\n",
    "        edgecolor='none',\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "# Plot noise counties explicitly in blue\n",
    "remaining_counties_noise = remaining_counties_fill[remaining_counties_fill['cluster_id'] == -1]\n",
    "if not remaining_counties_noise.empty:\n",
    "    remaining_counties_noise.plot(\n",
    "        facecolor='blue', # Explicitly set to blue for noise\n",
    "        linewidth=0.2,\n",
    "        ax=ax,\n",
    "        edgecolor='none',\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "# Additionally plot all outlines of the remaining counties (from the entire dataset)\n",
    "remaining_counties_all.plot(\n",
    "    facecolor='none',\n",
    "    linewidth=0.2,\n",
    "    ax=ax,\n",
    "    edgecolor='grey',\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Plot the state contours\n",
    "remaining_states.plot(\n",
    "    ax=ax,\n",
    "    color='none',\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "# 3. Plot Alaska in an inset\n",
    "# Define a bounding polygon for clipping\n",
    "polygon = Polygon([(-180, 50), (-180, 72), (-140, 72), (-140, 50)])\n",
    "alaska_state_clipped = alaska_state.clip(polygon)\n",
    "\n",
    "# Create the Alaska inset\n",
    "ak_ax = fig.add_axes([0.62, 0.255, 0.275, 0.275])\n",
    "\n",
    "# Plot non-noise Alaska counties with their assigned colors\n",
    "alaska_counties_fill_non_noise = alaska_counties_fill[alaska_counties_fill['cluster_id'] != -1]\n",
    "if not alaska_counties_fill_non_noise.empty:\n",
    "    alaska_counties_fill_non_noise = alaska_counties_fill_non_noise.clip(polygon)\n",
    "    alaska_counties_fill_non_noise.plot(\n",
    "        facecolor=alaska_counties_fill_non_noise['color'],\n",
    "        linewidth=0.2,\n",
    "        ax=ak_ax,\n",
    "        edgecolor='none',\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "# Plot noise Alaska counties explicitly in blue\n",
    "alaska_counties_fill_noise = alaska_counties_fill[alaska_counties_fill['cluster_id'] == -1]\n",
    "if not alaska_counties_fill_noise.empty:\n",
    "    alaska_counties_fill_noise = alaska_counties_fill_noise.clip(polygon)\n",
    "    alaska_counties_fill_noise.plot(\n",
    "        facecolor='blue', # Explicitly set to blue for noise\n",
    "        linewidth=0.2,\n",
    "        ax=ak_ax,\n",
    "        edgecolor='none',\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "# Plot all Alaska outlines (from the original dataset)\n",
    "if not alaska_counties_all.empty:\n",
    "    alaska_counties_all = alaska_counties_all.clip(polygon)\n",
    "    alaska_counties_all.plot(\n",
    "        facecolor='none',\n",
    "        linewidth=0.2,\n",
    "        ax=ak_ax,\n",
    "        edgecolor='grey',\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "# Plot the state contour of Alaska\n",
    "alaska_state_clipped.plot(\n",
    "    ax=ak_ax,\n",
    "    color='none',\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ak_ax.axis('off') # Hide axes in the Alaska inset\n",
    "ax.axis('off')    # Hide main axes\n",
    "\n",
    "unique_clusters = filtered_counties.groupby('cluster_id')['rank'].min().sort_values().index.tolist()\n",
    "cluster_counts = filtered_counties['cluster_id'].value_counts()\n",
    "legend_elements = []\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    if cluster == -1:\n",
    "        # Handle the \"noise\" cluster separately\n",
    "        count = cluster_counts.get(-1, 0)\n",
    "        legend_elements.append(Patch(facecolor='blue', label=f'C-1, ({count})'))\n",
    "    else:\n",
    "        # Determine the minimum rank for each cluster\n",
    "        min_rank = filtered_counties.loc[filtered_counties['cluster_id'] == cluster, 'rank'].min()\n",
    "        color = filtered_counties.loc[filtered_counties['cluster_id'] == cluster, 'color'].values[0]\n",
    "        count = cluster_counts.get(cluster, 0)\n",
    "        legend_elements.append(Patch(facecolor=color, label=f'C{cluster}, ({count})'))\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='lower left', bbox_to_anchor=(0.06, 0.095),\n",
    "          title_fontsize=12, fontsize=11, ncol=2)\n",
    "plt.savefig('county_cluster_map.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856acf6e-db5e-4305-a6de-e37e4288ebc8",
   "metadata": {},
   "source": [
    "## feature importance\n",
    "\n",
    "The code trains a separate Random Forest model for each unique cluster to assess the importance of various features in distinguishing each cluster.  \n",
    "The results are visualized in a heatmap, displaying the significance of each feature across different clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d56c5c-4265-4ec2-9b0d-08f31a5b6617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the style for visualization\n",
    "sns.set_context(\"paper\", font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Remove 'cluster_id', 'color', 'FIPS' etc.\n",
    "X = df_scaled[df_scaled.columns[1:-4]]\n",
    "clusters = sorted(df_scaled['cluster_id'].unique())\n",
    "\n",
    "# Calculate feature importances and scale them to integers\n",
    "feature_importances = [\n",
    "    {\"Cluster\": cluster, \"Feature\": feature, \"Importance\": int(importance * 100)}\n",
    "    for cluster in clusters\n",
    "    for feature, importance in zip(X.columns, RandomForestClassifier(random_state=42).fit(X, (df_scaled['cluster_id'] == cluster).astype(int)).feature_importances_)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame(feature_importances)\n",
    "\n",
    "# Pivot table for the heatmap\n",
    "pivot = importance_df.pivot(index=\"Feature\", columns=\"Cluster\", values=\"Importance\")\n",
    "\n",
    "# Sort clusters by 'rank'\n",
    "cluster_order = df_scaled.groupby('cluster_id')['rank'].mean().sort_values().index\n",
    "pivot = pivot[cluster_order]\n",
    "\n",
    "# Transform feature names (F1, F2, ...)\n",
    "feature_id_dict = {feature: f\"F{i+1}\" for i, feature in enumerate(df_original_features.columns)}\n",
    "pivot.index = pivot.index.map(lambda x: feature_id_dict.get(x, x))\n",
    "\n",
    "# Sort features in reverse numerical order\n",
    "pivot = pivot.sort_index(key=lambda x: x.str.extract(r'(\\d+)').astype(int)[0], ascending=False)\n",
    "\n",
    "# Draw the heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8), dpi=600)\n",
    "sns.heatmap(pivot, cmap=\"YlGnBu\", annot=True, fmt=\"d\",  # fmt=\"d\" for integers\n",
    "\n",
    "            annot_kws={\"size\": 10}, ax=ax)\n",
    "ax.set_xlabel('Cluster ID', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "plt.xticks(rotation=0, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and display the heatmap\n",
    "plt.savefig(\"feature_importance_heatmap.png\", format=\"png\", dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a50ceb-d766-493e-8d9f-fb919eb0b71e",
   "metadata": {},
   "source": [
    "## Barplots with std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c81d291-38f3-4d85-8d94-e387269d6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign 'cluster_id' and 'color' from df_scaled to df_original\n",
    "df_original['cluster_id'] = df_scaled['cluster_id']\n",
    "df_original['color'] = df_scaled['color']\n",
    "\n",
    "# Global settings for compact display\n",
    "plt.rcParams.update({\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 10,\n",
    "    'axes.labelsize': 10,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10\n",
    "})\n",
    "\n",
    "# Mapping for NAICS codes\n",
    "naics_plan_b = {\n",
    "    \"3315\": \"Foundries\",\n",
    "    \"Automotive\": \"Automotive\",\n",
    "    \"3364\": \"Aerospace\",\n",
    "    \"3366\": \"Shipbuilding\",\n",
    "    \"3335\": \"Metalworking Machines Manufacturing\",\n",
    "    \"3320A1\": \"Steel forming\",\n",
    "    \"3320A2\": \"Structural Metals Manufacturing\",\n",
    "    \"3327\": \"Machine Shops\",\n",
    "    \"3312\": \"Steel Product Manufacturing\",\n",
    "    \"3314\": \"Nonferrous Metal Production\"\n",
    "}\n",
    "\n",
    "# Mapping for Occupation codes\n",
    "occupations_plan_b = {\n",
    "    \"51-9022\": \"Grinding, Polishing by Hand\",\n",
    "    \"51-4121\": \"Welders, Cutters, Solderers\",\n",
    "    \"49-9041\": \"Industrial Machinery Mechanics\",\n",
    "    \"49-9071\": \"Maintenance and Repair Workers\",\n",
    "    \"51-4033\": \"Grinding, Lapping, Polishing\",\n",
    "    \"51-4035\": \"Milling and Planing Machine Setters\",\n",
    "    \"47-2211\": \"Sheet Metal Workers\",\n",
    "    \"51-2041\": \"Structural Metal Fabricators\"\n",
    "}\n",
    "\n",
    "# Lists with the desired codes\n",
    "naics_plan_b_keys = list(naics_plan_b.keys())\n",
    "occupations_plan_b_keys = list(occupations_plan_b.keys())\n",
    "\n",
    "# Cluster selection\n",
    "selected_clusters = [11,10,9,8,7]\n",
    "\n",
    "# Remove first and last column\n",
    "df = df_original.iloc[:, 1:-1]\n",
    "\n",
    "# Identify columns\n",
    "naics_cols = [col for col in df.columns if \"naics\" in col]\n",
    "occ_cols = [col for col in df.columns if \"occ\" in col]\n",
    "\n",
    "# Extract codes\n",
    "def extract_code(col_name):\n",
    "    match = re.search(r'(\\d{4,}A?\\d*|Automotive|\\d{2}-\\d{4})$', col_name)\n",
    "    return match.group() if match else None\n",
    "\n",
    "# Mapping: Original column names → Extracted codes\n",
    "occ_mapping = {col: extract_code(col) for col in occ_cols}\n",
    "naics_mapping = {col: extract_code(col) for col in naics_cols}\n",
    "\n",
    "# Keep only the desired codes from the lists\n",
    "occ_cols_filtered = [col for col, code in occ_mapping.items() if code in occupations_plan_b_keys]\n",
    "naics_cols_filtered = [col for col, code in naics_mapping.items() if code in naics_plan_b_keys]\n",
    "\n",
    "# Adjust labels accordingly\n",
    "occ_labels = [occupations_plan_b[occ_mapping[col]] for col in occ_cols_filtered]\n",
    "naics_labels = [naics_plan_b[naics_mapping[col]] for col in naics_cols_filtered]\n",
    "\n",
    "# Function to wrap label text a maximum of once (i.e., at most two lines)\n",
    "def wrap_label(label, width=15):\n",
    "    wrapped = textwrap.wrap(label, width=width)\n",
    "    if len(wrapped) > 1:\n",
    "        return wrapped[0] + \"\\n\" + wrapped[1]\n",
    "    else:\n",
    "        return wrapped[0]\n",
    "\n",
    "def wrap_label(label, width=19):\n",
    "    wrapped = textwrap.wrap(label, width=width)\n",
    "    return \"\\n\".join(wrapped[:3])  # maximum three lines\n",
    "\n",
    "wrapped_occ_labels = [wrap_label(label) for label in occ_labels]\n",
    "wrapped_naics_labels = [wrap_label(label) for label in naics_labels]\n",
    "\n",
    "# Calculate cluster means and standard deviations\n",
    "cluster_means = df.groupby(\"cluster_id\").mean()\n",
    "cluster_stds = df.groupby(\"cluster_id\").std()\n",
    "\n",
    "# Create a dictionary that assigns a color to each cluster\n",
    "cluster_colors = {}\n",
    "for cluster in selected_clusters:\n",
    "    color_val = df_original.loc[df_original[\"cluster_id\"] == cluster, \"color\"].iloc[0]\n",
    "    cluster_colors[cluster] = color_val\n",
    "\n",
    "# Compact parameters\n",
    "bar_width = 0.2    # narrower bars\n",
    "n_clusters = len(selected_clusters)\n",
    "group_gap = 0.3      # smaller spacing between feature groups\n",
    "\n",
    "# Calculate y-positions for OCC Features\n",
    "indices_occ = np.arange(len(wrapped_occ_labels)) * (n_clusters * bar_width + group_gap)\n",
    "y_ticks_occ = indices_occ + (n_clusters * bar_width) / 2\n",
    "\n",
    "# Calculate y-positions for NAICS Features\n",
    "indices_naics = np.arange(len(wrapped_naics_labels)) * (n_clusters * bar_width + group_gap)\n",
    "y_ticks_naics = indices_naics + (n_clusters * bar_width) / 2\n",
    "\n",
    "# Create the plot with a smaller overall size\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n",
    "\n",
    "# Error bar settings: thinner lines for the whiskers\n",
    "error_kw = {'elinewidth': 0.5, 'capsize': 3}\n",
    "\n",
    "# OCC Features Plot\n",
    "if occ_cols_filtered:\n",
    "    for i, cluster in enumerate(selected_clusters):\n",
    "        means = cluster_means.loc[cluster, occ_cols_filtered]\n",
    "        stds = cluster_stds.loc[cluster, occ_cols_filtered]\n",
    "        positions = indices_occ + i * bar_width\n",
    "        axes[0].barh(positions,\n",
    "                     means.values,\n",
    "                     height=bar_width,\n",
    "                     xerr=stds.values,\n",
    "                     label=f\"Cl. {cluster}\",\n",
    "                     error_kw=error_kw,\n",
    "                     color=cluster_colors[cluster])\n",
    "    axes[0].set_yticks(y_ticks_occ)\n",
    "    axes[0].set_yticklabels(wrapped_occ_labels, ha='left')\n",
    "    axes[0].set_title(\"Occupation features\")\n",
    "    axes[0].set_xlabel(\"mean value\")\n",
    "    axes[0].set_xlim(0, 3000)\n",
    "    axes[0].tick_params(axis='y', pad=105)\n",
    "    axes[0].legend(loc='upper right', frameon=False)\n",
    "\n",
    "# NAICS Features Plot\n",
    "if naics_cols_filtered:\n",
    "    for i, cluster in enumerate(selected_clusters):\n",
    "        means = cluster_means.loc[cluster, naics_cols_filtered]\n",
    "        stds = cluster_stds.loc[cluster, naics_cols_filtered]\n",
    "        positions = indices_naics + i * bar_width\n",
    "        axes[1].barh(positions,\n",
    "                     means.values,\n",
    "                     height=bar_width,\n",
    "                     xerr=stds.values,\n",
    "                     label=f\"Cl. {cluster}\",\n",
    "                     error_kw=error_kw,\n",
    "                     color=cluster_colors[cluster])\n",
    "    axes[1].set_yticks(y_ticks_naics)\n",
    "    axes[1].set_yticklabels(wrapped_naics_labels, ha='left')\n",
    "    axes[1].set_title(\"Industry features\")\n",
    "    axes[1].set_xlabel(\"mean value\")\n",
    "    axes[1].set_xlim(0, 2000)\n",
    "\n",
    "    axes[1].tick_params(axis='y', pad=90)\n",
    "    #axes[1].legend(loc='lower right', frameon=False) # This line is commented out in the original code, so keeping it commented.\n",
    "plt.tight_layout(w_pad=0.0)\n",
    "\n",
    "plt.savefig('cluster_feature_values.png', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ed81e-c18d-4ac5-b280-0a692f325ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d2bd6-9a46-4c33-aa7b-6482ed4e1d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10-DM+TF-Klon",
   "language": "python",
   "name": "py3.10-dm-tf-cloned"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
